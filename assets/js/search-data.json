{
  
    
  
    
        "post1": {
            "title": "Helping VCs target the next acquisition with > 90% confidence",
            "content": ". Overall Strategy . Using the Fast.AI library, I created shallow neural networks to encode all the categorical features of the funding data (company market, city, region, etc.) and then extracted the ebedding layers for further analysis with Machine Learning. . Instead of a simple label encoding of each feature, I created a vector representation. . The sample below shows a vector of length 3. The advantage is that we are able to transform discreet variables into vectors that are continous and meaningful. . &gt; &gt; ### . In the Fast.AI book, Deep Learning for Coders, an excellent example of the benefits of categorical embedding is provided. . On the left is a plot of the embedding matrix for the possible values of the State category. For a categorical variable we call the possible values of the variable its &quot;levels&quot; (or &quot;categories&quot; or &quot;classes&quot;), so here one level is &quot;Berlin,&quot; another is &quot;Hamburg,&quot; etc. On the right is a map of Germany. The actual physical locations of the German states were not part of the provided data, yet the model itself learned where they must be, based only on the behavior of store sales! . . The overall strategy to predict the success (the purchase of the startup) was the following: . Import and clean the data | Create additional features | Run data through multiple shallow neural networks to convert categorical data to continous | Train ML models on the latent space of the neural nets | Ensemble the predictions from each model | Train ML model on the Ensemble | Select the most confident prediction | Ensembling the embeddings . What makes this code unique is the fact that I took the same data and ran it through 4 different shallow neural networks with each network set to a different embedding size. . For each neural net, I extracted the latent layers and trained a Random Forest model on each embedding space and then ensembled the predictions and probabilites. The end result was an improvement of &gt; 6%. . The predictions improve because each model puts an emphasis on the different features. The two charts shown below show the top 10 features based on the importance to the model. Each model analyzes the data differently based on the relative importance of the features. Remember, the underlying data is identical, but I&#39;m analyzing the latent space with different embedding sizes. . . Import the data, split into Training and Test, Split into continous and categorical data . Run data through multiple shallow neural networks to convert categorical to continous data . Train ML models on the latent space of the neural nets . random.seed(1234) models = [ (&#39;RandomForest&#39;, RandomForestClassifier(max_depth=200, random_state=random.seed(1234), oob_score=True, min_samples_split=30)), ] procs_nn = [Categorify, FillMissing, Normalize] to_nn = TabularPandas(df_nn_final, procs_nn, cat_nn, cont_nn, splits=splits, y_names=dep_var) dls = to_nn.dataloaders(512) emb_szs={&#39;city&#39;:1, &#39;market&#39;:1,&#39;region&#39;:1,&#39;founded_month&#39;:1, &#39;founded_quarter&#39;:1,&#39;state_code&#39;:1} _, embeded_xs_1, xs_valid_1 = nn_embeddings(emb_szs) embed_1_df = run_exps(models, embeded_xs_1, y_train, xs_valid_1, y_test, &#39;embed_1&#39;) emb_szs={&#39;city&#39;:2, &#39;market&#39;:2,&#39;region&#39;:2,&#39;founded_month&#39;:2, &#39;founded_quarter&#39;:2,&#39;state_code&#39;:2} _, embeded_xs_2, xs_valid_2 = nn_embeddings(emb_szs) embed_2_df = run_exps(models, embeded_xs_2, y_train, xs_valid_2, y_test, &#39;embed_2&#39;) emb_szs={&#39;city&#39;:3, &#39;market&#39;:3,&#39;region&#39;:3,&#39;founded_month&#39;:3, &#39;founded_quarter&#39;:3,&#39;state_code&#39;:3} _, embeded_xs_3, xs_valid_3 = nn_embeddings(emb_szs) embed_3_df = run_exps(models, embeded_xs_3, y_train, xs_valid_3, y_test, &#39;embed_3&#39;) _, embeded_xs_default, xs_valid_default = nn_embeddings() embed_default_df = run_exps(models, embeded_xs_default, y_train, xs_valid_default, y_test, &#39;default_embedding&#39;) . epoch train_loss valid_loss accuracy time . 0 | 2.105271 | 4.765329 | 0.561782 | 00:00 | . 1 | 2.424031 | 2.302882 | 0.567529 | 00:00 | . 2 | 1.865460 | 27.719856 | 0.564655 | 00:00 | . No improvement since epoch 0: early stopping (&#39;RandomForest&#39;, &#39;embed_1&#39;) precision recall f1-score support closed 0.72 0.57 0.64 305 aquired 0.71 0.82 0.76 391 accuracy 0.71 696 macro avg 0.71 0.70 0.70 696 weighted avg 0.71 0.71 0.71 696 . epoch train_loss valid_loss accuracy time . 0 | 0.681131 | 0.682984 | 0.612069 | 00:00 | . 1 | 0.652406 | 0.663289 | 0.653736 | 00:00 | . 2 | 0.627544 | 0.655687 | 0.668103 | 00:00 | . No improvement since epoch 0: early stopping (&#39;RandomForest&#39;, &#39;embed_2&#39;) precision recall f1-score support closed 0.69 0.60 0.64 305 aquired 0.72 0.79 0.75 391 accuracy 0.71 696 macro avg 0.71 0.70 0.70 696 weighted avg 0.71 0.71 0.71 696 . epoch train_loss valid_loss accuracy time . 0 | 0.681013 | 0.680430 | 0.623563 | 00:00 | . 1 | 0.643252 | 0.643235 | 0.662356 | 00:00 | . 2 | 0.613667 | 0.623342 | 0.675287 | 00:00 | . No improvement since epoch 0: early stopping (&#39;RandomForest&#39;, &#39;embed_3&#39;) precision recall f1-score support closed 0.71 0.60 0.65 305 aquired 0.72 0.81 0.77 391 accuracy 0.72 696 macro avg 0.72 0.71 0.71 696 weighted avg 0.72 0.72 0.72 696 . epoch train_loss valid_loss accuracy time . 0 | 0.677926 | 0.684507 | 0.561782 | 00:00 | . 1 | 0.628663 | 0.674555 | 0.632184 | 00:00 | . 2 | 0.576841 | 0.665309 | 0.635057 | 00:00 | . No improvement since epoch 0: early stopping (&#39;RandomForest&#39;, &#39;default_embedding&#39;) precision recall f1-score support closed 0.71 0.62 0.66 305 aquired 0.73 0.81 0.77 391 accuracy 0.72 696 macro avg 0.72 0.71 0.71 696 weighted avg 0.72 0.72 0.72 696 . The ensemble of the latent layers improves the model by &gt; 6%. To reach 95% accuracy, I focus on the top 20% of the companies based on the confidence of the model. . Ensemble the predictions from each model . #combine all predictions into single dataframe and train with RF del combined_predictions combined_predictions = pd.concat([embed_1_df, embed_2_df, embed_3_df, embed_default_df ], axis=1) combined_predictions = combined_predictions.T.drop_duplicates().T combined_predictions.set_index(&#39;company_index&#39;, inplace=True) combined_predictions.drop([&#39;company_index&#39;],axis=1,inplace=True, errors=&#39;ignore&#39;) combined_predictions.drop([&#39;combined_confidence&#39;],axis=1,inplace=True, errors=&#39;ignore&#39;) # add new feature summing predictions # combined_predictions[&#39;combined_confidence&#39;] = combined_predictions.delta_RandomForest_aquired_embed_1_RandomForest_closed_embed_1 + # combined_predictions.delta_RandomForest_aquired_embed_2_RandomForest_closed_embed_2 + # combined_predictions.delta_RandomForest_aquired_embed_3_RandomForest_closed_embed_3 + # combined_predictions.delta_RandomForest_aquired_default_embedding_RandomForest_closed_default_embedding #now need to split d2 into train and valid sets X_train_combined, X_test_combined, y_train_combined, y_test_combined = train_test_split(combined_predictions, y_test, test_size=test_size, random_state=random.seed(1234)) . . (&#39;RF2&#39;, &#39;combined_model_predictions&#39;) precision recall f1-score support closed 0.73 0.65 0.68 62 aquired 0.74 0.81 0.77 78 accuracy 0.74 140 macro avg 0.73 0.73 0.73 140 weighted avg 0.74 0.74 0.73 140 . RF2_closed_combined_model_predictions RF2_aquired_combined_model_predictions delta_RF2_aquired_combined_model_predictions_RF2_closed_combined_model_predictions company_index . 0 -1.458795 | -0.264638 | 1.194157 | 271.0 | . 1 -0.861403 | -0.549166 | 0.312236 | 584.0 | . 2 -0.530403 | -0.887625 | -0.357222 | 536.0 | . 3 -0.670106 | -0.716732 | -0.046627 | 892.0 | . 4 -1.230013 | -0.345719 | 0.884294 | 428.0 | . ... ... | ... | ... | ... | . 135 -0.423224 | -1.064013 | -0.640789 | 1464.0 | . 136 -2.802899 | -0.062550 | 2.740349 | 714.0 | . 137 -0.920011 | -0.508353 | 0.411658 | 2117.0 | . 138 -0.402139 | -1.105297 | -0.703158 | 592.0 | . 139 -0.956728 | -0.484748 | 0.471979 | 2081.0 | . 140 rows × 4 columns . Train ML model on the Ensemble . #clf = RandomForestClassifier(max_depth=3, random_state=random.seed(1234), oob_score=False, min_samples_split=100, n_estimators=5) clf = RandomForestClassifier(max_depth=200, random_state=random.seed(1234), oob_score=True, min_samples_split=30) clf.fit(X_train_combined, y_train_combined) print (clf.score(X_test_combined, y_test_combined)) . 0.7428571428571429 . Select the most confident prediction . predicted_val=clf.predict_proba(X_test_combined) predicted=clf.predict(X_test_combined) pred_df=pd.DataFrame(predicted_val, columns=[&#39;z&#39;,&#39;o&#39;]) #pred_df[&#39;names&#39;]=(X_test_combined.company_index.values) pred_df[&#39;target&#39;]=y_test_combined.values pred_df[&#39;names&#39;]=y_test_combined.index pred_df[&#39;predict_acquisition&#39;]=predicted pred_df[&#39;diff&#39;] = pred_df.apply(lambda x: x[&#39;o&#39;] - x[&#39;z&#39;], axis=1) confident_pred_df=pred_df[(pred_df[&#39;diff&#39;]&gt;.60) | (pred_df[&#39;diff&#39;]&lt;-.60)] confident_pred_df.set_index(&#39;names&#39;,inplace=True) final_acquisition_predictions = pd.merge(company_names, confident_pred_df, how = &#39;right&#39;,left_index = True, right_index = True) final_acquisition_predictions.drop([&#39;o&#39;,&#39;z&#39;], inplace=True, axis=1, ) final_acquisition_predictions.index.name = &#39;idx_name&#39; final_acquisition_predictions.sort_values(by=&#39;diff&#39;) . . name target predict_acquisition diff . idx_name . 202 Apps Genius | False | False | -0.926551 | . 3359 WHObyYOU | False | False | -0.902164 | . 1356 IDverge | False | False | -0.872564 | . 2071 Oakland Single Parents&#39; Network | False | False | -0.828960 | . 2370 PRSM Healthcare | False | False | -0.826424 | . 1929 My eShoe | False | False | -0.677138 | . 2090 OKDJ.fm | False | False | -0.642144 | . 2263 Pivot | True | True | 0.611366 | . 1740 Marcadia Biotech | True | True | 0.612383 | . 1662 Lockdown Networks | True | True | 0.616012 | . 2557 Sabrix | True | True | 0.630229 | . 67 Adify | True | True | 0.632446 | . 1387 IndexTank | True | True | 0.634539 | . 1632 LimeLife | True | True | 0.647528 | . 1915 mSnap | True | True | 0.660241 | . 3279 VirtualSharp Software | True | True | 0.661401 | . 635 Color Labs Inc. | False | True | 0.672413 | . 1330 Hyper9 | True | True | 0.678118 | . 2615 SeeWhy | True | True | 0.697798 | . 3069 Tiny Pictures | True | True | 0.738538 | . 2775 SocialMedia.com | True | True | 0.783984 | . 76 Adometry By Google | True | True | 0.790112 | . 1084 Framehawk | True | True | 0.811959 | . 93 AdXpose | True | True | 0.869579 | . 3305 Vontu | True | True | 0.887197 | . 335 Berkeley Design Automation | True | True | 0.888731 | . 714 Crescendo Networks | True | True | 0.893015 | . 3309 Voxify | True | True | 0.899999 | . 2020 Next New Networks | True | True | 0.916480 | . 863 Dropcam | True | True | 0.916480 | . 2520 Ripple TV | True | True | 0.956856 | . 1604 LeisureLink | True | True | 0.958483 | . Interesting high confidence predictions . I predicted that Kickfire was aquired with a 79% confidence, however the data provided by Crunchbase listed it as a closed. After investigation, Kickfire was actually aquired in 2010 by Teradata. Details can be found in this article. . Similarly, I predicted Color Labs Inc. was aquired, but the data provided said it closed. Apple aquired Color Labs Inc. for $7M in 2012. . The other company that I predicted would get aquired is Breaktime Studios. This company went under in 2016. . While I was able to make accurate predictions on the limited data of market type and the aount of funding in each round. It would have been helpful to know who was making the investments. Was it credible VCs with a known trackrecord or individuals. Stay tuined while I add this to the model. . cm=confusion_matrix(final_acquisition_predictions.target, final_acquisition_predictions.predict_acquisition, labels=None, sample_weight=None, normalize=None) plot_confusion_matrix(cm, [&#39;closed&#39;,&#39;aquired&#39;], normalize=None) . .",
            "url": "https://patrick-hanley.github.io/thought_experiments/2021/02/05/fastai_crunchbase.html",
            "relUrl": "/2021/02/05/fastai_crunchbase.html",
            "date": " • Feb 5, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://patrick-hanley.github.io/thought_experiments/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://patrick-hanley.github.io/thought_experiments/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://patrick-hanley.github.io/thought_experiments/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}