{
  
    
        "post0": {
            "title": "Can we predict which startup will be aquired?",
            "content": ". %reload_ext autoreload %autoreload 2 %matplotlib inline !pip install fastai --upgrade --q from pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype from fastai.tabular.all import * from sklearn.ensemble import RandomForestClassifier from sklearn.tree import DecisionTreeClassifier from xgboost import XGBClassifier from sklearn import model_selection from sklearn.utils import class_weight from sklearn.metrics import classification_report from sklearn.metrics import confusion_matrix from IPython.display import Image, display_svg, SVG from sklearn.model_selection import train_test_split from google.colab import drive drive.mount(&#39;/content/gdrive&#39;, force_remount=True) root_dir = &quot;/content/gdrive/My Drive/&quot; . . |████████████████████████████████| 194kB 6.0MB/s |████████████████████████████████| 61kB 4.6MB/s Mounted at /content/gdrive . base_dir = root_dir + &#39;crunchbase&#39; path = Path(base_dir) csv_path = Path(base_dir+&#39;/crunchbase_data_cleaned_3.csv&#39;) df = pd.read_csv(csv_path) cleanup_nums = {&quot;status&quot;: {2: True, 1:False}} df = df.replace(cleanup_nums) X=df.copy() y=X.status X.drop(&#39;status&#39;,inplace=True, axis=1) categorical_features_indices = np.where(X.dtypes == &#39;object&#39;)[0] dep_var = &#39;status&#39; cont,cat = cont_cat_split(df, 1, dep_var=dep_var) procs = [Categorify, FillMissing] # split data into train and test sets seed = 7 test_size = 0.2 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed) splits = (list(X_train.index),list(X_test.index)) to = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits) xs, y = to.train.xs,to.train.y valid_xs, valid_y = to.valid.xs,to.valid.y . . A neural network is created and the embedding layers are extracted for each categorical feature. . Instead of a simple label encoding of each feature, we create a vector representation. The sample below shows a vector of length 3. . The advantage is that we are able to transform discreet variable into vectors that are continous and meaningful. . &gt; ### . In the Fast.AI book, Deep Learning for Coders, an excellent example of the benefits of categorical embedding is provided. . On the left is a plot of the embedding matrix for the possible values of the State category. For a categorical variable we call the possible values of the variable its &quot;levels&quot; (or &quot;categories&quot; or &quot;classes&quot;), so here one level is &quot;Berlin,&quot; another is &quot;Hamburg,&quot; etc. On the right is a map of Germany. The actual physical locations of the German states were not part of the provided data, yet the model itself learned where they must be, based only on the behavior of store sales! . The overall strategy to predict the success (the purchase of the startup) was the following: . Import and clean the data | Creaate additional features | Run data through a neural network to convert categorical data to continous | Train ML models on the latent space of the neural network | Select the most confident prediction | def feat_importance(m, df): # git list of feature importance for given model try: return pd.DataFrame({&#39;cols&#39;:df.columns, &#39;imp&#39;:m.feature_importances_} ).sort_values(&#39;imp&#39;, ascending=False) except: return &quot;&quot; def save_predictions(name, m, df, target_names, note): col = [] for target_name in target_names: tmp = &quot;_&quot;.join([name, target_name, note]) col.append(tmp) try: predictions = m.predict_log_proba(df) except: predictions = m.predict_proba(df) predict_df = pd.DataFrame(predictions, columns=col).astype(&quot;float&quot;) delta_name = &quot;_&quot;.join([&#39;delta&#39;, col[1], col[0]]) predict_df[delta_name] = predict_df[col[1]] - predict_df[col[0]] return predict_df # modified based on this blog: https://towardsdatascience.com/quickly-test-multiple-models-a98477476f0 def run_exps(models, X_train: pd.DataFrame , y_train: pd.DataFrame, X_test: pd.DataFrame, y_test: pd.DataFrame, note) -&gt; pd.DataFrame: &#39;&#39;&#39; Lightweight script to test many models and find winners :param models: array of tuples containing (model_name, model) :param X_train: training split :param y_train: training target vector :param X_test: test split :param y_test: test target vector :return: DataFrame of predictions &#39;&#39;&#39; print (X_train.shape, y_train.shape, X_test.shape, y_test.shape) dfs = [] dfs_predictions = [] results = [] names = [] scoring = [&#39;accuracy&#39;, &#39;precision_weighted&#39;, &#39;recall_weighted&#39;, &#39;f1_weighted&#39;, &#39;roc_auc&#39;] target_names = [&#39;closed&#39;, &#39;aquired&#39;] for name, model in models: kfold = model_selection.KFold(n_splits=5, shuffle=True, random_state=90210) cv_results = model_selection.cross_validate(model, X_train, y_train, cv=kfold, scoring=scoring) clf = model.fit(X_train, y_train) y_pred = clf.predict(X_test) print(name) print(classification_report(y_test, y_pred, target_names=target_names)) try: fi=feat_importance(clf, X_test) #print (fi[:10]) plot_fi(name, fi[:10]); except: print() print () predict_df=save_predictions(name, clf, X_test, target_names, note) #print (predict_df[:10]) results.append(cv_results) names.append(name) this_df = pd.DataFrame(cv_results) this_df[&#39;model&#39;] = name dfs.append(this_df) dfs_predictions.append(predict_df) final = pd.concat(dfs, ignore_index=True) prediction_dfs = pd.concat(dfs_predictions, axis=1) return final, prediction_dfs def nn_embeddings(city_es = 3, market_es = 3, region_es = 6, founded_month_es = 1, founded_quarter_es = 8, state_code_es = 7, use_default_embeddings=False): # use a neural network to create ebeddings of categorical data if use_default_embeddings==False: learn = tabular_learner(dls,layers=[200,100],metrics=accuracy , emb_szs={&#39;city&#39;:city_es, &#39;market&#39;:market_es, &#39;region&#39;:region_es, &#39;founded_month&#39;:founded_month_es, &#39;founded_quarter&#39;:founded_quarter_es, &#39;state_code&#39;:state_code_es}) else: learn = tabular_learner(dls,layers=[200,100], metrics=accuracy) lr_in, lr_steep = learn.lr_find(show_plot=False) learn.fit_one_cycle(20, lr_steep, cbs=EarlyStoppingCallback(monitor=&#39;accuracy&#39;, min_delta=0.1, patience=2)) # after fitting, get embeddings to be used by ML embeded_xs = embed_features(learn, learn.dls.train.xs) xs_valid = embed_features(learn, learn.dls.valid.xs) return (learn, embeded_xs, xs_valid) def embed_features(learner, xs): # https://forums.fast.ai/t/using-embedding-from-the-neural-network-in-random-forests/80063/9 # citation: danielwbn xs = xs.copy() for i, feature in enumerate(learner.dls.cat_names): emb = learner.model.embeds[i] new_feat = pd.DataFrame(emb(tensor(xs[feature], dtype=torch.int64)), index=xs.index, columns=[f&#39;{feature}_{j}&#39; for j in range(emb.embedding_dim)]) xs.drop(columns=feature, inplace=True) xs = xs.join(new_feat) return xs def plot_fi(name, fi): return fi.plot(&#39;cols&#39;, &#39;imp&#39;, &#39;barh&#39;, figsize=(12,7), legend=False, title=name) . . Ensembling the embeddings . What makes this code unique is the fact that I took the same data and ran it through 4 different neural networks with each one set to a different embedding size. . For each neural network, I extracted the latent layers and trained both Random Forest and XGBoost models on each embedding space and then ensembled the predictions and probabilites. The end result was an improvement of &gt;6%. . print(csv_path) df = pd.read_csv(csv_path) cleanup_nums = {&quot;status&quot;: {2: True, 1:False}} df = df.replace(cleanup_nums) company_names = df.name # For use later, but not necessary to use it for learning city_names=df.city y=df.status X=df.copy() X.drop([&#39;status&#39;,&#39;name&#39;],axis=1,inplace=True) # use reduced columns indicated by feature importance use_reduced = False dep_var = &#39;status&#39; def use_reduced_features(use_reduced): if use_reduced: return (df[list(xs_imp.columns) + [dep_var]]) return df[list(X.columns) + [dep_var]] df_nn_final = use_reduced_features (use_reduced) y=df_nn_final.status X=df_nn_final.copy() X.drop([&#39;status&#39;],axis=1,inplace=True) seed = 7 test_size = 0.2 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed) splits = (list(X_train.index),list(X_test.index)) df_nn_final = df_nn_final.astype({&#39;funding_total_usd&#39;: &#39;int64&#39;, &#39;first_funding_year&#39;:&#39;int64&#39;, &#39;founded_year&#39;:&#39;int64&#39;, &#39;last_funding_year&#39;:&#39;int64&#39;, &#39;funding_rounds&#39;:&#39;int64&#39;, &#39;venture&#39;: &#39;int64&#39;}) cont_nn,cat_nn = cont_cat_split(df_nn_final, max_card=10, dep_var=dep_var) . /content/gdrive/My Drive/crunchbase/crunchbase_data_cleaned_3.csv . models = [ # (&#39;RF&#39;, RandomForestClassifier()), (&#39;RF2&#39;, RandomForestClassifier(max_depth=200, random_state=0, oob_score=True, min_samples_split=30)), #(&#39;KNN&#39;, KNeighborsClassifier()), #(&#39;SVM&#39;, SVC(probability=True)), #(&#39;GNB&#39;, GaussianNB()), # (&#39;XGB&#39;, XGBClassifier()), # (&#39;XGB2&#39;, XGBClassifier(max_depth=3,n_estimators=100,learning_rate=.01)) ] procs_nn = [Categorify, FillMissing, Normalize] to_nn = TabularPandas(df_nn_final, procs_nn, cat_nn, cont_nn, splits=splits, y_names=dep_var) dls = to_nn.dataloaders(512) _, embeded_xs_1, xs_valid_1 = nn_embeddings(city_es = 1, market_es = 1, region_es = 1, founded_month_es = 1, founded_quarter_es =1, state_code_es =1) df_f, embed_1_df = run_exps(models, embeded_xs_1, y_train, xs_valid_1, valid_y, &#39;embed_1&#39;) _, embeded_xs_2, xs_valid_2 = nn_embeddings(city_es = 2, market_es = 2, region_es = 2, founded_month_es = 2, founded_quarter_es =2, state_code_es =2) df_f, embed_2_df = run_exps(models, embeded_xs_2, y_train, xs_valid_2, valid_y, &#39;embed_2&#39;) _, embeded_xs_hand, xs_valid_hand = nn_embeddings(city_es = 3, market_es = 3, region_es = 3, founded_month_es = 3, founded_quarter_es =3, state_code_es =3) df_f, embed_hand_df = run_exps(models, embeded_xs_hand, y_train, xs_valid_hand, valid_y, &#39;embed_hand&#39;) _, embeded_xs_default, xs_valid_default = nn_embeddings(use_default_embeddings=True) df_f, embed_default_df = run_exps(models, embeded_xs_default, y_train, xs_valid_default, valid_y, &#39;embed_default&#39;) . epoch train_loss valid_loss accuracy time . 0 | 0.678079 | 0.661896 | 0.625000 | 00:00 | . 1 | 0.643613 | 0.627760 | 0.655172 | 00:00 | . 2 | 0.618727 | 0.628606 | 0.678161 | 00:00 | . No improvement since epoch 0: early stopping (2784, 54) (2784,) (696, 54) (696,) RF2 precision recall f1-score support closed 0.71 0.57 0.63 308 aquired 0.71 0.81 0.75 388 accuracy 0.71 696 macro avg 0.71 0.69 0.69 696 weighted avg 0.71 0.71 0.70 696 . epoch train_loss valid_loss accuracy time . 0 | 0.669494 | 0.655065 | 0.645115 | 00:00 | . 1 | 0.634903 | 0.631305 | 0.662356 | 00:00 | . 2 | 0.599768 | 0.638665 | 0.659483 | 00:00 | . No improvement since epoch 0: early stopping (2784, 60) (2784,) (696, 60) (696,) RF2 precision recall f1-score support closed 0.69 0.61 0.65 308 aquired 0.72 0.78 0.75 388 accuracy 0.71 696 macro avg 0.71 0.70 0.70 696 weighted avg 0.71 0.71 0.71 696 . epoch train_loss valid_loss accuracy time . 0 | 0.671778 | 0.670082 | 0.632184 | 00:00 | . 1 | 0.640863 | 0.637376 | 0.649425 | 00:00 | . 2 | 0.610807 | 0.627586 | 0.659483 | 00:00 | . No improvement since epoch 0: early stopping (2784, 66) (2784,) (696, 66) (696,) RF2 precision recall f1-score support closed 0.70 0.61 0.66 308 aquired 0.72 0.79 0.76 388 accuracy 0.71 696 macro avg 0.71 0.70 0.71 696 weighted avg 0.71 0.71 0.71 696 . epoch train_loss valid_loss accuracy time . 0 | 0.692161 | 0.685768 | 0.603448 | 00:00 | . 1 | 0.641813 | 0.659333 | 0.642241 | 00:00 | . 2 | 0.590462 | 0.632749 | 0.633621 | 00:00 | . No improvement since epoch 0: early stopping (2784, 229) (2784,) (696, 229) (696,) RF2 precision recall f1-score support closed 0.67 0.60 0.63 308 aquired 0.70 0.76 0.73 388 accuracy 0.69 696 macro avg 0.69 0.68 0.68 696 weighted avg 0.69 0.69 0.69 696 . final3 = pd.concat([embed_1_df, embed_2_df, embed_hand_df, embed_default_df ], axis=1) . # procs_nn = [Categorify, FillMissing, Normalize] # dls = to_nn.dataloaders(512) X_train2, X_test2, y_train2, y_test2 = train_test_split(final3, y_test, test_size=test_size, random_state=seed) splits2 = (list(X_train2.index),list(X_test2.index)) # to_nn = TabularPandas(final3, procs_nn, cat_nn, cont_nn, # splits=splits2, y_names=dep_var) models = [ # (&#39;RF&#39;, RandomForestClassifier()), (&#39;RF2&#39;, RandomForestClassifier(max_depth=200, random_state=0, oob_score=True, min_samples_split=30)), # (&#39;KNN&#39;, KNeighborsClassifier()), #(&#39;SVM&#39;, SVC(probability=True)), #(&#39;GNB&#39;, GaussianNB()), # (&#39;XGB&#39;, XGBClassifier()), # (&#39;XGB2&#39;, XGBClassifier(max_depth=3,n_estimators=100,learning_rate=.01)) ] new_X =np.nan_to_num(X_train2.astype(np.float32)) new_X_test2 = np.nan_to_num(X_test2.astype(np.float32)) df_f3, d3 = run_exps(models, new_X, y_train2, new_X_test2, y_test2, &#39;final3&#39;) . (556, 12) (556,) (140, 12) (140,) RF2 precision recall f1-score support closed 0.69 0.70 0.70 54 aquired 0.81 0.80 0.81 86 accuracy 0.76 140 macro avg 0.75 0.75 0.75 140 weighted avg 0.77 0.76 0.76 140 . clf = RandomForestClassifier(max_depth=200, random_state=0, oob_score=True, min_samples_split=30) clf.fit(new_X, y_train2) print (clf.score(new_X_test2, y_test2)) . 0.7642857142857142 . new_X_test2[:2] . RF_closed_embed_1 RF_aquired_embed_1 delta_RF_aquired_embed_1_RF_closed_embed_1 RF2_closed_embed_1 RF2_aquired_embed_1 delta_RF2_aquired_embed_1_RF2_closed_embed_1 XGB_closed_embed_1 XGB_aquired_embed_1 delta_XGB_aquired_embed_1_XGB_closed_embed_1 XGB2_closed_embed_1 XGB2_aquired_embed_1 delta_XGB2_aquired_embed_1_XGB2_closed_embed_1 RF_closed_embed_2 RF_aquired_embed_2 delta_RF_aquired_embed_2_RF_closed_embed_2 RF2_closed_embed_2 RF2_aquired_embed_2 delta_RF2_aquired_embed_2_RF2_closed_embed_2 XGB_closed_embed_2 XGB_aquired_embed_2 delta_XGB_aquired_embed_2_XGB_closed_embed_2 XGB2_closed_embed_2 XGB2_aquired_embed_2 delta_XGB2_aquired_embed_2_XGB2_closed_embed_2 RF_closed_embed_hand RF_aquired_embed_hand delta_RF_aquired_embed_hand_RF_closed_embed_hand RF2_closed_embed_hand RF2_aquired_embed_hand delta_RF2_aquired_embed_hand_RF2_closed_embed_hand XGB_closed_embed_hand XGB_aquired_embed_hand delta_XGB_aquired_embed_hand_XGB_closed_embed_hand XGB2_closed_embed_hand XGB2_aquired_embed_hand delta_XGB2_aquired_embed_hand_XGB2_closed_embed_hand RF_closed_embed_default RF_aquired_embed_default delta_RF_aquired_embed_default_RF_closed_embed_default RF2_closed_embed_default RF2_aquired_embed_default delta_RF2_aquired_embed_default_RF2_closed_embed_default XGB_closed_embed_default XGB_aquired_embed_default delta_XGB_aquired_embed_default_XGB_closed_embed_default XGB2_closed_embed_default XGB2_aquired_embed_default delta_XGB2_aquired_embed_default_XGB2_closed_embed_default . 471 -0.150823 | -1.966113 | -1.815290 | -0.127974 | -2.119230 | -1.991255 | 0.912050 | 0.087950 | -0.824099 | 0.698719 | 0.301281 | -0.397437 | -0.210721 | -1.660731 | -1.450010 | -0.244399 | -1.528666 | -1.284267 | 0.858032 | 0.141968 | -0.716063 | 0.613603 | 0.386397 | -0.227206 | -0.105361 | -2.302585 | -2.197225 | -0.116078 | -2.210971 | -2.094893 | 0.978529 | 0.021471 | -0.957059 | 0.771098 | 0.228902 | -0.542196 | -0.094311 | -2.407946 | -2.313635 | -0.167226 | -1.870858 | -1.703632 | 0.892988 | 0.107012 | -0.785976 | 0.691211 | 0.308789 | -0.382422 | . 424 -0.314711 | -1.309333 | -0.994623 | -0.280258 | -1.408904 | -1.128646 | 0.722743 | 0.277257 | -0.445486 | 0.576801 | 0.423199 | -0.153602 | -0.223144 | -1.609438 | -1.386294 | -0.280479 | -1.408220 | -1.127741 | 0.652113 | 0.347887 | -0.304227 | 0.581345 | 0.418655 | -0.162691 | -0.210721 | -1.660731 | -1.450010 | -0.263936 | -1.461115 | -1.197179 | 0.674241 | 0.325759 | -0.348482 | 0.585724 | 0.414276 | -0.171448 | -0.415515 | -1.078810 | -0.663294 | -0.453847 | -1.008350 | -0.554503 | 0.563749 | 0.436251 | -0.127498 | 0.537384 | 0.462616 | -0.074767 | . company_names.iloc[471] . &#39;Callidus Biopharma&#39; . predicted_val=clf.predict_log_proba(new_X_test2) predicted=clf.predict(new_X_test2) def get_max(x): if x[&#39;diff&#39;]&gt;0: x.p=1 else: x.p=0 return x pred_df=pd.DataFrame(predicted_val, columns=[&#39;z&#39;,&#39;o&#39;]) pred_df[&#39;names&#39;]=(X_test2.index) pred_df[&#39;tar&#39;]=y_test2.values#.numpy() pred_df[&#39;p&#39;]=y_test2#.numpy() pred_df[&#39;pred&#39;]=predicted pred_df[&#39;diff&#39;] = pred_df.apply(lambda x: x[&#39;o&#39;] - x[&#39;z&#39;], axis=1) pred_df=pred_df.apply(get_max, axis=1) #pred_df=pred_df[np.abs(pred_df[&#39;diff&#39;])&gt;1.4] #pred_df=pred_df[(pred_df[&#39;diff&#39;]&gt;1.7) | (pred_df[&#39;diff&#39;]&lt;-1.58)] #93 with 4 models pred_df=pred_df[(pred_df[&#39;diff&#39;]&gt;1.8) | (pred_df[&#39;diff&#39;]&lt;-1.58)] #92 with 1 model . pred_df.sort_values(by=&#39;diff&#39;)[:60] . z o names tar p pred diff . 89 -0.040390 | -3.229297 | 501 | False | 0 | False | -3.188907 | . 73 -0.063014 | -2.795743 | 508 | False | 0 | False | -2.732730 | . 9 -0.083135 | -2.528567 | 223 | False | 0 | False | -2.445431 | . 123 -0.083595 | -2.523273 | 646 | False | 0 | False | -2.439677 | . 52 -0.099706 | -2.354967 | 683 | False | 0 | False | -2.255261 | . 10 -0.101428 | -2.338693 | 531 | False | 0 | False | -2.237265 | . 0 -0.120860 | -2.172947 | 471 | False | 0 | False | -2.052087 | . 19 -0.124148 | -2.147715 | 9 | False | 0 | False | -2.023568 | . 44 -0.150028 | -1.971010 | 261 | False | 0 | False | -1.820982 | . 2 -0.171228 | -1.849150 | 418 | False | 0 | False | -1.677921 | . 93 -0.185170 | -1.777636 | 504 | False | 0 | False | -1.592465 | . 49 -1.974314 | -0.149494 | 593 | True | 1 | True | 1.824820 | . 23 -2.012157 | -0.143524 | 343 | True | 1 | True | 1.868633 | . 136 -2.249945 | -0.111384 | 46 | True | 1 | True | 2.138561 | . 85 -2.285811 | -0.107242 | 609 | True | 1 | True | 2.178569 | . 41 -2.293122 | -0.106418 | 278 | False | 1 | True | 2.186704 | . 138 -2.297498 | -0.105927 | 179 | True | 1 | True | 2.191571 | . 36 -2.553618 | -0.080993 | 339 | True | 1 | True | 2.472626 | . 17 -2.658580 | -0.072622 | 138 | True | 1 | True | 2.585958 | . 60 -2.712336 | -0.068687 | 159 | True | 1 | True | 2.643648 | . 70 -2.960863 | -0.053163 | 64 | False | 1 | True | 2.907700 | . 108 -3.179964 | -0.042477 | 395 | True | 1 | True | 3.137487 | . 28 -3.489132 | -0.031003 | 327 | True | 1 | True | 3.458129 | . 24 -3.515185 | -0.030194 | 217 | True | 1 | True | 3.484992 | . cm=confusion_matrix(pred_df.tar, pred_df.p, labels=None, sample_weight=None, normalize=None) plot_confusion_matrix(cm, [&#39;closed&#39;,&#39;aquired&#39;], normalize=None) . decision tree . clf = DecisionTreeClassifier(random_state=0, min_samples_leaf=35) clf.fit(to.train.xs, to.train.y) clf.score(to.train.xs, to.train.y), clf.score(to.valid.xs,to.valid.y) . (0.7385057471264368, 0.6451149425287356) . fi = rf_feat_importance(clf, to.valid.xs) fi[:10] . cols imp . 12 venture | 0.347140 | . 8 funding_total_usd | 0.105861 | . 33 last_funding_year | 0.070974 | . 51 count_fund_for_market_and_first_funding_year | 0.054537 | . 26 round_C | 0.053379 | . 41 mean_fund_for_city | 0.034380 | . 42 median_fund_for_city | 0.033763 | . 38 median_fund_for_market | 0.032099 | . 25 round_B | 0.030925 | . 49 mean_fund_for_market_and_first_funding_year | 0.025557 | . clf = RandomForestClassifier(max_depth=200, random_state=0, oob_score=True, min_samples_split=30) clf.fit(to.train.xs, to.train.y) clf.score(to.train.xs, to.train.y), clf.score(to.valid.xs,to.valid.y) . (0.8635057471264368, 0.735632183908046) . fi = rf_feat_importance(clf, to.valid.xs) fi[:10] . cols imp . 12 venture | 0.065824 | . 8 funding_total_usd | 0.063624 | . 35 ave_rounds_per_year | 0.053932 | . 40 delta_total_fund_median_for_market | 0.053554 | . 44 delta_total_fund_median_for_city | 0.043044 | . 51 count_fund_for_market_and_first_funding_year | 0.038372 | . 42 median_fund_for_city | 0.032805 | . 41 mean_fund_for_city | 0.031759 | . 48 delta_total_fund_median_for_first_funding_year | 0.030985 | . 0 name | 0.029487 | . def plot_fi(fi): return fi.plot(&#39;cols&#39;, &#39;imp&#39;, &#39;barh&#39;, figsize=(12,7), legend=False) plot_fi(fi[:10]); . to_keep = fi[fi.imp &gt;.02].cols print (len(to.valid.xs.columns), len(to_keep)) xs_imp = xs[to_keep] valid_xs_imp = valid_xs[to_keep] clf.fit(xs_imp, to.train.y) clf.score(xs_imp, to.train.y), clf.score(valid_xs_imp,to.valid.y) . NameError Traceback (most recent call last) &lt;ipython-input-25-0659c3e77e32&gt; in &lt;module&gt;() 1 #to_keep = fi[fi.imp&gt;0.005].cols -&gt; 2 to_keep = fi[fi.imp &gt;.02].cols 3 print (len(to.valid.xs.columns), len(to_keep)) 4 5 xs_imp = xs[to_keep] NameError: name &#39;fi&#39; is not defined . list(xs_imp.columns), xs_imp.shape . ([&#39;venture&#39;, &#39;funding_total_usd&#39;, &#39;ave_rounds_per_year&#39;, &#39;delta_total_fund_median_for_market&#39;, &#39;delta_total_fund_median_for_city&#39;, &#39;count_fund_for_market_and_first_funding_year&#39;, &#39;median_fund_for_city&#39;, &#39;mean_fund_for_city&#39;, &#39;delta_total_fund_median_for_first_funding_year&#39;, &#39;name&#39;, &#39;delta_total_fund_mean_for_market&#39;, &#39;delta_total_fund_mean_for_city&#39;, &#39;delta_total_fund_median_for_market_and_first_funding_year&#39;, &#39;delta_total_fund_mean_for_market_and_first_funding_year&#39;, &#39;median_fund_for_market_and_first_funding_year&#39;, &#39;last_funding_year&#39;, &#39;round_A&#39;, &#39;first_funding_year&#39;, &#39;round_B&#39;, &#39;funding_rounds&#39;, &#39;city&#39;, &#39;mean_fund_for_market_and_first_funding_year&#39;, &#39;market&#39;, &#39;seed&#39;, &#39;delta_total_fund_mean_for_first_funding_year&#39;], (2784, 25)) . NN comparison . print(csv_path) df = pd.read_csv(csv_path) cleanup_nums = {&quot;status&quot;: {2: True, 1:False}} df = df.replace(cleanup_nums) company_names = df.name # For use later, but not necessary to use it for learning city_names=df.city y=df.status X=df.copy() X.drop([&#39;status&#39;,&#39;name&#39;],axis=1,inplace=True) # use reduced columns indicated by feature importance use_reduced = False dep_var = &#39;status&#39; def use_reduced_features(use_reduced): if use_reduced: return (df[list(xs_imp.columns) + [dep_var]]) return df[list(X.columns) + [dep_var]] df_nn_final = use_reduced_features (use_reduced) y=df_nn_final.status X=df_nn_final.copy() X.drop([&#39;status&#39;],axis=1,inplace=True) seed = 7 test_size = 0.2 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed) splits = (list(X_train.index),list(X_test.index)) df_nn_final = df_nn_final.astype({&#39;funding_total_usd&#39;: &#39;int64&#39;, &#39;first_funding_year&#39;:&#39;int64&#39;, &#39;founded_year&#39;:&#39;int64&#39;, &#39;last_funding_year&#39;:&#39;int64&#39;, &#39;funding_rounds&#39;:&#39;int64&#39;, &#39;venture&#39;: &#39;int64&#39;}) cont_nn,cat_nn = cont_cat_split(df_nn_final, max_card=10, dep_var=dep_var) . /content/gdrive/My Drive/crunchbase/crunchbase_data_cleaned_3.csv . city_names[:4] . 0 New York 1 Oakland Park 2 San Francisco 3 San Francisco Name: city, dtype: object . use_reduced = False dep_var = &#39;status&#39; def use_reduced_features(use_reduced): if use_reduced: return (df[list(xs_imp.columns) + [dep_var]]) return df[list(X.columns) + [dep_var]] df_nn_final = use_reduced_features (use_reduced) y=df_nn_final.status X=df_nn_final.copy() X.drop([&#39;status&#39;],axis=1,inplace=True) . . seed = 7 test_size = 0.2 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed) splits = (list(X_train.index),list(X_test.index)) . X_train.shape . (2784, 53) . df_nn_final.columns . Index([&#39;market&#39;, &#39;funding_total_usd&#39;, &#39;country_code&#39;, &#39;state_code&#39;, &#39;region&#39;, &#39;city&#39;, &#39;funding_rounds&#39;, &#39;founded_month&#39;, &#39;founded_quarter&#39;, &#39;founded_year&#39;, &#39;seed&#39;, &#39;venture&#39;, &#39;equity_crowdfunding&#39;, &#39;undisclosed&#39;, &#39;convertible_note&#39;, &#39;debt_financing&#39;, &#39;angel&#39;, &#39;grant&#39;, &#39;private_equity&#39;, &#39;post_ipo_equity&#39;, &#39;post_ipo_debt&#39;, &#39;secondary_market&#39;, &#39;product_crowdfunding&#39;, &#39;round_A&#39;, &#39;round_B&#39;, &#39;round_C&#39;, &#39;round_D&#39;, &#39;round_E&#39;, &#39;round_F&#39;, &#39;round_G&#39;, &#39;round_H&#39;, &#39;first_funding_year&#39;, &#39;last_funding_year&#39;, &#39;delta_first_fund_founded&#39;, &#39;ave_rounds_per_year&#39;, &#39;delta_seed_to_Round_A&#39;, &#39;mean_fund_for_market&#39;, &#39;median_fund_for_market&#39;, &#39;delta_total_fund_mean_for_market&#39;, &#39;delta_total_fund_median_for_market&#39;, &#39;mean_fund_for_city&#39;, &#39;median_fund_for_city&#39;, &#39;delta_total_fund_mean_for_city&#39;, &#39;delta_total_fund_median_for_city&#39;, &#39;mean_fund_for_first_funding_year&#39;, &#39;median_fund_for_first_funding_year&#39;, &#39;delta_total_fund_mean_for_first_funding_year&#39;, &#39;delta_total_fund_median_for_first_funding_year&#39;, &#39;mean_fund_for_market_and_first_funding_year&#39;, &#39;median_fund_for_market_and_first_funding_year&#39;, &#39;count_fund_for_market_and_first_funding_year&#39;, &#39;delta_total_fund_mean_for_market_and_first_funding_year&#39;, &#39;delta_total_fund_median_for_market_and_first_funding_year&#39;, &#39;status&#39;], dtype=&#39;object&#39;) . df_nn_final.columns . Index([&#39;market&#39;, &#39;funding_total_usd&#39;, &#39;country_code&#39;, &#39;state_code&#39;, &#39;region&#39;, &#39;city&#39;, &#39;funding_rounds&#39;, &#39;founded_month&#39;, &#39;founded_quarter&#39;, &#39;founded_year&#39;, &#39;seed&#39;, &#39;venture&#39;, &#39;equity_crowdfunding&#39;, &#39;undisclosed&#39;, &#39;convertible_note&#39;, &#39;debt_financing&#39;, &#39;angel&#39;, &#39;grant&#39;, &#39;private_equity&#39;, &#39;post_ipo_equity&#39;, &#39;post_ipo_debt&#39;, &#39;secondary_market&#39;, &#39;product_crowdfunding&#39;, &#39;round_A&#39;, &#39;round_B&#39;, &#39;round_C&#39;, &#39;round_D&#39;, &#39;round_E&#39;, &#39;round_F&#39;, &#39;round_G&#39;, &#39;round_H&#39;, &#39;first_funding_year&#39;, &#39;last_funding_year&#39;, &#39;delta_first_fund_founded&#39;, &#39;ave_rounds_per_year&#39;, &#39;delta_seed_to_Round_A&#39;, &#39;mean_fund_for_market&#39;, &#39;median_fund_for_market&#39;, &#39;delta_total_fund_mean_for_market&#39;, &#39;delta_total_fund_median_for_market&#39;, &#39;mean_fund_for_city&#39;, &#39;median_fund_for_city&#39;, &#39;delta_total_fund_mean_for_city&#39;, &#39;delta_total_fund_median_for_city&#39;, &#39;mean_fund_for_first_funding_year&#39;, &#39;median_fund_for_first_funding_year&#39;, &#39;delta_total_fund_mean_for_first_funding_year&#39;, &#39;delta_total_fund_median_for_first_funding_year&#39;, &#39;mean_fund_for_market_and_first_funding_year&#39;, &#39;median_fund_for_market_and_first_funding_year&#39;, &#39;count_fund_for_market_and_first_funding_year&#39;, &#39;delta_total_fund_mean_for_market_and_first_funding_year&#39;, &#39;delta_total_fund_median_for_market_and_first_funding_year&#39;, &#39;status&#39;], dtype=&#39;object&#39;) . df_nn_final = df_nn_final.astype({&#39;funding_total_usd&#39;: &#39;int64&#39;, &#39;first_funding_year&#39;:&#39;int64&#39;, &#39;founded_year&#39;:&#39;int64&#39;, &#39;last_funding_year&#39;:&#39;int64&#39;, &#39;funding_rounds&#39;:&#39;int64&#39;, &#39;venture&#39;: &#39;int64&#39;}) cont_nn,cat_nn = cont_cat_split(df_nn_final, max_card=10, dep_var=dep_var) . . . . . print(df_nn_final[cat_nn].nunique()) print() print (df_nn_final[cont_nn].nunique()) . market 347 country_code 1 state_code 47 region 142 city 582 founded_month 152 founded_quarter 54 dtype: int64 funding_total_usd 1522 funding_rounds 11 founded_year 14 seed 245 venture 1160 equity_crowdfunding 5 undisclosed 12 convertible_note 36 debt_financing 282 angel 103 grant 37 private_equity 77 post_ipo_equity 25 post_ipo_debt 9 secondary_market 1 product_crowdfunding 3 round_A 294 round_B 253 round_C 161 round_D 101 round_E 40 round_F 18 round_G 2 round_H 1 first_funding_year 16 last_funding_year 14 delta_first_fund_founded 23 ave_rounds_per_year 1641 delta_seed_to_Round_A 18 mean_fund_for_market 347 median_fund_for_market 256 delta_total_fund_mean_for_market 3006 delta_total_fund_median_for_market 2379 mean_fund_for_city 572 median_fund_for_city 468 delta_total_fund_mean_for_city 2955 delta_total_fund_median_for_city 2629 mean_fund_for_first_funding_year 16 median_fund_for_first_funding_year 16 delta_total_fund_mean_for_first_funding_year 2384 delta_total_fund_median_for_first_funding_year 1882 mean_fund_for_market_and_first_funding_year 1003 median_fund_for_market_and_first_funding_year 757 count_fund_for_market_and_first_funding_year 144 delta_total_fund_mean_for_market_and_first_funding_year 3210 delta_total_fund_median_for_market_and_first_funding_year 2577 dtype: int64 . procs_nn = [Categorify, FillMissing, Normalize] to_nn = TabularPandas(df_nn_final, procs_nn, cat_nn, cont_nn, splits=splits, y_names=dep_var) . dls = to_nn.dataloaders(512) . learn = tabular_learner(dls,layers=[2000,1000],metrics=accuracy)# ,emb_szs={&#39;city&#39;:6, &#39;market&#39;:4}) . lr_in, lr_steep = learn.lr_find() . learn.fit_one_cycle(5, .002) . epoch train_loss valid_loss accuracy time . 0 | 0.659387 | 0.649514 | 0.633621 | 00:01 | . 1 | 0.615312 | 0.646420 | 0.617816 | 00:01 | . 2 | 0.542827 | 0.638471 | 0.646552 | 00:01 | . 3 | 0.476651 | 0.644053 | 0.646552 | 00:01 | . 4 | 0.420533 | 0.678714 | 0.630747 | 00:01 | . df_f = run_exps(embeded_xs, y_train, xs_valid, y_test) . RF precision recall f1-score support closed 0.63 0.56 0.59 308 aquired 0.68 0.74 0.71 388 accuracy 0.66 696 macro avg 0.66 0.65 0.65 696 weighted avg 0.66 0.66 0.66 696 cols imp 0 funding_total_usd 0.025680 4 venture 0.023617 27 ave_rounds_per_year 0.021719 32 delta_total_fund_median_for_market 0.021278 40 delta_total_fund_median_for_first_funding_year 0.014058 RF2 precision recall f1-score support closed 0.67 0.58 0.62 308 aquired 0.70 0.77 0.73 388 accuracy 0.69 696 macro avg 0.68 0.67 0.68 696 weighted avg 0.68 0.69 0.68 696 cols imp 0 funding_total_usd 0.036097 4 venture 0.030581 27 ave_rounds_per_year 0.027457 32 delta_total_fund_median_for_market 0.026040 145 city_15 0.021108 KNN precision recall f1-score support closed 0.62 0.57 0.59 308 aquired 0.68 0.72 0.70 388 accuracy 0.65 696 macro avg 0.65 0.64 0.64 696 weighted avg 0.65 0.65 0.65 696 SVM precision recall f1-score support closed 0.67 0.60 0.63 308 aquired 0.71 0.76 0.73 388 accuracy 0.69 696 macro avg 0.69 0.68 0.68 696 weighted avg 0.69 0.69 0.69 696 GNB precision recall f1-score support closed 0.45 0.97 0.62 308 aquired 0.72 0.06 0.11 388 accuracy 0.46 696 macro avg 0.58 0.52 0.36 696 weighted avg 0.60 0.46 0.33 696 XGB precision recall f1-score support closed 0.63 0.59 0.61 308 aquired 0.69 0.73 0.71 388 accuracy 0.67 696 macro avg 0.66 0.66 0.66 696 weighted avg 0.66 0.67 0.66 696 cols imp 4 venture 0.067996 0 funding_total_usd 0.036419 145 city_15 0.021212 87 market_41 0.019661 184 city_54 0.016172 . _, embeded_xs, xs_valid = nn_embeddings(city_es = 1, market_es = 1, region_es = 1, founded_month_es = 1, founded_quarter_es =1, state_code_es =1) df_f = run_exps(embeded_xs, y_train, xs_valid, y_test) . 0.0009120108559727668 0.004365158267319202 . epoch train_loss valid_loss accuracy time . 0 | 0.671427 | 0.645250 | 0.665230 | 00:01 | . 1 | 0.644202 | 0.630360 | 0.663793 | 00:01 | . 2 | 0.622600 | 0.615674 | 0.673851 | 00:01 | . No improvement since epoch 0: early stopping RF precision recall f1-score support closed 0.68 0.59 0.63 308 aquired 0.71 0.78 0.74 388 accuracy 0.70 696 macro avg 0.69 0.68 0.69 696 weighted avg 0.69 0.70 0.69 696 cols imp 27 ave_rounds_per_year 0.053051 0 funding_total_usd 0.046052 4 venture 0.041930 51 city_0 0.038575 32 delta_total_fund_median_for_market 0.038441 RF2 precision recall f1-score support closed 0.71 0.61 0.66 308 aquired 0.72 0.81 0.76 388 accuracy 0.72 696 macro avg 0.72 0.71 0.71 696 weighted avg 0.72 0.72 0.71 696 cols imp 0 funding_total_usd 0.065156 4 venture 0.060923 27 ave_rounds_per_year 0.055207 32 delta_total_fund_median_for_market 0.052983 51 city_0 0.044130 KNN precision recall f1-score support closed 0.61 0.57 0.59 308 aquired 0.68 0.72 0.70 388 accuracy 0.65 696 macro avg 0.65 0.64 0.64 696 weighted avg 0.65 0.65 0.65 696 SVM precision recall f1-score support closed 0.67 0.60 0.63 308 aquired 0.71 0.76 0.73 388 accuracy 0.69 696 macro avg 0.69 0.68 0.68 696 weighted avg 0.69 0.69 0.69 696 GNB precision recall f1-score support closed 0.45 0.97 0.61 308 aquired 0.69 0.05 0.09 388 accuracy 0.46 696 macro avg 0.57 0.51 0.35 696 weighted avg 0.58 0.46 0.32 696 XGB precision recall f1-score support closed 0.70 0.58 0.64 308 aquired 0.71 0.80 0.75 388 accuracy 0.71 696 macro avg 0.71 0.69 0.70 696 weighted avg 0.71 0.71 0.70 696 cols imp 4 venture 0.146114 0 funding_total_usd 0.099838 18 round_C 0.060344 17 round_B 0.047768 25 last_funding_year 0.035367 . from sklearn.metrics import classification_report . def embed_features(learner, xs): # https://forums.fast.ai/t/using-embedding-from-the-neural-network-in-random-forests/80063/9 # citation: danielwbn xs = xs.copy() for i, feature in enumerate(learn.dls.cat_names): emb = learner.model.embeds[i] new_feat = pd.DataFrame(emb(tensor(xs[feature], dtype=torch.int64)), index=xs.index, columns=[f&#39;{feature}_{j}&#39; for j in range(emb.embedding_dim)]) xs.drop(columns=feature, inplace=True) xs = xs.join(new_feat) return xs . embeded_xs = embed_features(learn, learn.dls.train.xs) xs_valid = embed_features(learn, learn.dls.valid.xs) . df_out = pd.merge(company_names, embeded_xs, how = &#39;right&#39;,left_index = True, right_index = True) df_out2 = pd.merge(city_names, df_out, how = &#39;right&#39;,left_index = True, right_index = True) df_out2[:2] . city name funding_total_usd funding_rounds founded_year seed venture equity_crowdfunding undisclosed convertible_note debt_financing angel grant private_equity post_ipo_equity post_ipo_debt secondary_market product_crowdfunding round_A round_B round_C round_D round_E round_F round_G round_H first_funding_year last_funding_year delta_first_fund_founded ave_rounds_per_year delta_seed_to_Round_A mean_fund_for_market median_fund_for_market delta_total_fund_mean_for_market delta_total_fund_median_for_market mean_fund_for_city median_fund_for_city delta_total_fund_mean_for_city delta_total_fund_median_for_city mean_fund_for_first_funding_year ... founded_month_2 founded_month_3 founded_month_4 founded_month_5 founded_month_6 founded_month_7 founded_month_8 founded_month_9 founded_month_10 founded_month_11 founded_month_12 founded_month_13 founded_month_14 founded_month_15 founded_month_16 founded_month_17 founded_month_18 founded_month_19 founded_month_20 founded_month_21 founded_month_22 founded_month_23 founded_month_24 founded_month_25 founded_month_26 founded_quarter_0 founded_quarter_1 founded_quarter_2 founded_quarter_3 founded_quarter_4 founded_quarter_5 founded_quarter_6 founded_quarter_7 founded_quarter_8 founded_quarter_9 founded_quarter_10 founded_quarter_11 founded_quarter_12 founded_quarter_13 founded_quarter_14 . 37 San Francisco | Acendi Interactive | -0.137297 | -0.752694 | 0.445981 | -0.261841 | -0.357561 | -0.024792 | -0.031719 | -0.060202 | -0.066855 | -0.163837 | -0.032854 | -0.075613 | -0.024524 | -0.026264 | 0.0 | -0.018956 | -0.29693 | -0.111819 | -0.273011 | -0.166394 | -0.067034 | -0.033377 | -0.018956 | 0.0 | 0.049577 | -0.478425 | -0.636856 | -0.165169 | 0.053991 | -0.027671 | 0.723271 | -0.136181 | -0.153981 | -0.240497 | -0.284279 | -0.11132 | -0.119931 | 0.024109 | ... | 0.013669 | 0.01509 | -0.003822 | -0.015373 | -0.006393 | 0.017485 | -0.007078 | 0.011269 | -0.004001 | 0.003036 | -0.008594 | -0.007402 | -0.008876 | 0.008349 | 0.006263 | 0.012332 | 0.009981 | 0.010618 | -0.012018 | -0.003986 | -0.006738 | -0.01037 | 0.004821 | 0.015092 | -0.001168 | -0.011245 | -0.004537 | 0.005212 | 0.011617 | -0.005559 | 0.007004 | 0.007672 | 0.011503 | 0.001904 | -0.000933 | -0.007103 | 0.003357 | 0.014095 | 0.008256 | 0.006791 | . 890 Boston | eegoes | -0.137239 | 0.673301 | 0.772035 | -0.261841 | -0.357309 | -0.024792 | -0.031719 | -0.060202 | -0.066855 | -0.163837 | -0.032854 | -0.075613 | -0.024524 | -0.026264 | 0.0 | -0.018956 | -0.29693 | -0.171430 | -0.273011 | -0.166394 | -0.067034 | -0.033377 | -0.018956 | 0.0 | 0.441685 | 0.366176 | -0.636856 | -0.194551 | 0.053991 | -1.138351 | -0.834174 | -0.043653 | -0.118211 | -0.128788 | -0.182533 | -0.12460 | -0.126178 | 0.330408 | ... | 0.013669 | 0.01509 | -0.003822 | -0.015373 | -0.006393 | 0.017485 | -0.007078 | 0.011269 | -0.004001 | 0.003036 | -0.008594 | -0.007402 | -0.008876 | 0.008349 | 0.006263 | 0.012332 | 0.009981 | 0.010618 | -0.012018 | -0.003986 | -0.006738 | -0.01037 | 0.004821 | 0.015092 | -0.001168 | -0.011245 | -0.004537 | 0.005212 | 0.011617 | -0.005559 | 0.007004 | 0.007672 | 0.011503 | 0.001904 | -0.000933 | -0.007103 | 0.003357 | 0.014095 | 0.008256 | 0.006791 | . 2 rows × 231 columns . clf = RandomForestClassifier(max_depth=200, random_state=0, oob_score=True, min_samples_split=30) clf.fit(embeded_xs, learn.dls.targ.values.ravel()) clf.score(embeded_xs, learn.dls.targ), clf.score(xs_valid, y_test) . (0.920617816091954, 0.6853448275862069) . cat values to embed . city 582 market 347 region 142 founded_month 152 founded_quarter 54 state_code 47 . columns = [&#39;ebedding_size&#39;, &#39;trn_score&#39;, &#39;val_score&#39;] + cat_nn ebedded_df=pd.DataFrame(columns=columns) . embedding_sizes = [0,1,2,4,8,16,32,64,128] embedding_sizes = [5,6,7,8,9,10] embedding_sizes =[2,3,4,5,6,7,8,16,32,64,128,1] embedding_sizes =[3] for inc, es in enumerate(embedding_sizes): city_es = 3 market_es = 3 region_es = 6 founded_month_es = 1 founded_quarter_es = 32 state_code_es = 7 #es city_es = es market_es = es region_es = es founded_month_es =es founded_quarter_es = es state_code_es = es #es #clf = RandomForestClassifier(max_depth=200, random_state=0, oob_score=True, min_samples_split=30) clf = RandomForestClassifier(max_depth=20, random_state=0, oob_score=True, min_samples_split=25) learn = tabular_learner(dls,layers=[2000,1000],metrics=accuracy , emb_szs={&#39;city&#39;:city_es, #+2, &#39;market&#39;:market_es, &#39;region&#39;:region_es, &#39;founded_month&#39;:founded_month_es, &#39;founded_quarter&#39;:founded_quarter_es, &#39;state_code&#39;:state_code_es}) learn.lr_find() learn.fit_one_cycle(6, .0001, cbs=EarlyStoppingCallback(monitor=&#39;accuracy&#39;, min_delta=0.1, patience=6)) embeded_xs = embed_features(learn, learn.dls.train.xs) xs_valid = embed_features(learn, learn.dls.valid.xs) clf.fit(embeded_xs, learn.dls.targ) trn=clf.score(embeded_xs, learn.dls.targ) val=clf.score(xs_valid, y_test) ebedded_df.at[inc, &#39;ebedding_size&#39;] = es ebedded_df.at[inc, &#39;trn_score&#39;] = trn ebedded_df.at[inc, &#39;val_score&#39;] = val fi = rf_feat_importance(clf, xs_valid) print(es, trn, val) for feature in cat_nn: col_fi=fi[fi[&#39;cols&#39;].str.contains(feature)] # market contains 13.2% of the important features with embedding of 8 col_sum = col_fi.imp.sum() print (es, feature, col_sum ) ebedded_df.at[inc, feature] = col_sum ebedded_df . epoch train_loss valid_loss accuracy time . 0 | 0.686024 | 0.674187 | 0.633621 | 00:01 | . 1 | 0.656219 | 0.651108 | 0.646552 | 00:01 | . 2 | 0.635543 | 0.636986 | 0.666667 | 00:01 | . 3 | 0.619859 | 0.644567 | 0.668103 | 00:01 | . 4 | 0.606075 | 0.637931 | 0.655172 | 00:01 | . 5 | 0.597032 | 0.637052 | 0.647988 | 00:01 | . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:37: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). . NameError Traceback (most recent call last) &lt;ipython-input-57-0fa533982c41&gt; in &lt;module&gt;() 39 val=clf.score(xs_valid, y_test) 40 &gt; 41 ebedded_df.at[inc, &#39;ebedding_size&#39;] = es 42 ebedded_df.at[inc, &#39;trn_score&#39;] = trn 43 ebedded_df.at[inc, &#39;val_score&#39;] = val NameError: name &#39;ebedded_df&#39; is not defined . ebedded_df . ebedding_size trn_score val_score market country_code state_code region city founded_month founded_quarter . 0 3 | 0.897989 | 0.711207 | 0.256115 | 0 | 0.0285116 | 0.0448933 | 0.167306 | 0.0448656 | 0.0368689 | . ebedded_df.diff() . ebedding_size trn_score val_score market country_code state_code region city founded_month founded_quarter . 0 NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . fi = rf_feat_importance(clf, xs_valid) plot_fi(fi[:40]); # Feature importance with embedding size of 8 . fi = rf_feat_importance(clf, xs_valid) plot_fi(fi[:10]); # Feature importance with embedding size of 1 . cat_nn . [&#39;market&#39;, &#39;country_code&#39;, &#39;state_code&#39;, &#39;region&#39;, &#39;city&#39;, &#39;founded_month&#39;, &#39;founded_quarter&#39;] . fi[fi[&#39;cols&#39;].str.contains(&quot;market&quot;)]. # market contains 5% of the important features with embedding of 1 . File &#34;&lt;ipython-input-57-0a00d4ccdaa0&gt;&#34;, line 1 fi[fi[&#39;cols&#39;].str.contains(&#34;market&#34;)]. # market contains 5% of the important features with embedding of 1 ^ SyntaxError: invalid syntax . market_fi=fi[fi[&#39;cols&#39;].str.contains(&quot;market&quot;)] # market contains 13.2% of the important features with embedding of 8 market_fi.imp.sum() . 0.25611533692606486 . market_fi=fi[fi[&#39;cols&#39;].str.contains(&quot;market&quot;)] # market contains 13.2% of the important features with embedding of 16 market_fi.imp.sum() . compare df of nn and random foresst . aare they different? - No, they are the same . remove embeddings 1 by 1 until get closer, . needed to remove embeddings to see improvement (WTF????) . remove similar items? . https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py . https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance_multicollinear.html . remove embeddings and get models for dt, rf, and xgb . d.shape, company_names.shape . ((2784, 1), (3480,)) . _, embeded_xs, xs_valid = nn_embeddings(city_es = 4, market_es = 4, region_es = 4, founded_month_es = 4, founded_quarter_es = 4, state_code_es = 4) d = embeded_xs.loc[:, embeded_xs.columns.str.startswith(&#39;city&#39;)] . epoch train_loss valid_loss accuracy time . 0 | 0.708100 | 0.689830 | 0.584770 | 00:01 | . 1 | 0.694734 | 0.669623 | 0.632184 | 00:01 | . 2 | 0.675995 | 0.645350 | 0.649425 | 00:01 | . 3 | 0.656206 | 0.632086 | 0.658046 | 00:01 | . 4 | 0.640234 | 0.625763 | 0.655172 | 00:01 | . 5 | 0.624650 | 0.622616 | 0.658046 | 00:01 | . 6 | 0.610305 | 0.620619 | 0.662356 | 00:01 | . 7 | 0.596849 | 0.622656 | 0.660920 | 00:01 | . 8 | 0.584531 | 0.630324 | 0.660920 | 00:01 | . 9 | 0.572497 | 0.632818 | 0.660920 | 00:01 | . 10 | 0.560901 | 0.628097 | 0.666667 | 00:01 | . No improvement since epoch 0: early stopping . pca = decomposition.PCA(n_components=2) pca.fit(d) X = pca.transform(d) fig = px.scatter(x=X[:, 0], y=X[:, 1], color=y.values, marginal_y=&quot;violin&quot;)#, text=df_out2.city, # marginal_x=&quot;box&quot;, trendline=&quot;ols&quot;) fig.show() . . . # xs_valid[&#39;city&#39;]=valid_xs.city # embeded_xs[&#39;market&#39;]=xs.market # xs_valid[&#39;market&#39;]=valid_xs.market . def drop_embedded(df, name, max_int): for i in range(max_int): val=name+&quot;_&quot;+str(i) try: df.drop(val, axis=1, inplace=True) except: #do nothing print(&#39;.&#39;,end=&#39;&#39;) #drop_embedded(embeded_xs, &#39;city&#39;, 70) #drop_embedded(xs_valid, &#39;city&#39;, 70) . #drop_embedded(xs_valid, &#39;market&#39;, 70) # for col in cat_nn: # drop_embedded(embeded_xs, col, 70) # drop_embedded(xs_valid, col, 70) . _, embeded_xs, xs_valid = nn_embeddings(city_es = 1, market_es = 1, region_es = 1, founded_month_es = 1, founded_quarter_es = 1, state_code_es = 1) . epoch train_loss valid_loss accuracy time . 0 | 0.720352 | 0.689464 | 0.528736 | 00:01 | . 1 | 0.705340 | 0.669657 | 0.604885 | 00:01 | . 2 | 0.684471 | 0.643429 | 0.650862 | 00:01 | . 3 | 0.667143 | 0.628821 | 0.652299 | 00:01 | . 4 | 0.651465 | 0.614593 | 0.675287 | 00:01 | . 5 | 0.636255 | 0.608347 | 0.679598 | 00:01 | . 6 | 0.623014 | 0.598896 | 0.692529 | 00:01 | . 7 | 0.610538 | 0.599043 | 0.689655 | 00:01 | . 8 | 0.599648 | 0.593200 | 0.673851 | 00:01 | . 9 | 0.590757 | 0.595158 | 0.683908 | 00:01 | . 10 | 0.582108 | 0.600327 | 0.693965 | 00:01 | . 11 | 0.573995 | 0.598504 | 0.691092 | 00:01 | . 12 | 0.566959 | 0.601552 | 0.688218 | 00:01 | . No improvement since epoch 2: early stopping . # clf.fit(embeded_xs, learn.dls.targ.values.ravel()) # print (clf.score(xs_valid, y_test)) clf = RandomForestClassifier(max_depth=900, random_state=0, oob_score=True, min_samples_split=95)#, n_estimators=400) clf.fit(embeded_xs, learn.dls.targ.values.ravel()) clf.score(embeded_xs, learn.dls.targ), clf.score(xs_valid, y_test) #rf_prob_no_ebedding = clf.predict_proba(xs_valid) rf_prob_no_ebedding = clf.predict_log_proba(xs_valid) prf_prob_no_ebedding = pd.DataFrame(rf_prob_no_ebedding, columns=[&#39;rf_0_no_ebedding&#39;,&#39;rf_1_no_ebedding&#39;]).astype(&quot;float&quot;) prf_prob_no_ebedding[:4] . rf_0_no_ebedding rf_1_no_ebedding . 0 -0.715404 | -0.671375 | . 1 -0.885880 | -0.531626 | . 2 -1.170952 | -0.371168 | . 3 -1.097743 | -0.405900 | . validation_company_names = pd.merge(company_names, xs_valid, how = &#39;right&#39;,left_index = True, right_index = True)[&#39;name&#39;] validation_company_names=validation_company_names.reset_index() validation_company_names.drop(&#39;index&#39;,axis=1,inplace=True) . XGBoost . import xgboost as xgb from xgboost import XGBClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score . model = XGBClassifier(max_depth=3,n_estimators=100,learning_rate=.01) model.fit(embeded_xs, learn.dls.targ.values.ravel()) print(model, model.score(embeded_xs, learn.dls.targ), model.score(xs_valid, y_test)) #xgb_prob_no_ebedding = clf.predict_proba(xs_valid) xgb_prob_no_ebedding = clf.predict_log_proba(xs_valid) pxgb_prob_no_ebedding = pd.DataFrame(xgb_prob_no_ebedding, columns=[&#39;xgb_0_no_ebedding&#39;,&#39;xgb_1_no_ebedding&#39;]).astype(&quot;float&quot;) . XGBClassifier(base_score=0.5, booster=&#39;gbtree&#39;, colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.01, max_delta_step=0, max_depth=3, min_child_weight=1, missing=None, n_estimators=100, n_jobs=1, nthread=None, objective=&#39;binary:logistic&#39;, random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None, silent=None, subsample=1, verbosity=1) 0.7079741379310345 0.7370689655172413 . found best by adjusting embedding size. . now do PCA of train and test . https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.html . ultimately, want to pca successful and inprogress to find next company purchase . from sklearn import decomposition . embeded_xs[:2] . funding_total_usd funding_rounds founded_year seed venture equity_crowdfunding undisclosed convertible_note debt_financing angel grant private_equity post_ipo_equity post_ipo_debt secondary_market product_crowdfunding round_A round_B round_C round_D round_E round_F round_G round_H first_funding_year last_funding_year delta_first_fund_founded ave_rounds_per_year delta_seed_to_Round_A mean_fund_for_market median_fund_for_market delta_total_fund_mean_for_market delta_total_fund_median_for_market mean_fund_for_city median_fund_for_city delta_total_fund_mean_for_city delta_total_fund_median_for_city mean_fund_for_first_funding_year median_fund_for_first_funding_year delta_total_fund_mean_for_first_funding_year delta_total_fund_median_for_first_funding_year mean_fund_for_market_and_first_funding_year median_fund_for_market_and_first_funding_year count_fund_for_market_and_first_funding_year delta_total_fund_mean_for_market_and_first_funding_year delta_total_fund_median_for_market_and_first_funding_year market_0 country_code_0 country_code_1 state_code_0 region_0 city_0 founded_month_0 founded_quarter_0 . 37 -0.137297 | -0.752694 | 0.445981 | -0.261841 | -0.357561 | -0.024792 | -0.031719 | -0.060202 | -0.066855 | -0.163837 | -0.032854 | -0.075613 | -0.024524 | -0.026264 | 0.0 | -0.018956 | -0.29693 | -0.111819 | -0.273011 | -0.166394 | -0.067034 | -0.033377 | -0.018956 | 0.0 | 0.049577 | -0.478425 | -0.636856 | -0.165169 | 0.053991 | -0.027671 | 0.723271 | -0.136181 | -0.153981 | -0.240497 | -0.284279 | -0.11132 | -0.119931 | 0.024109 | -0.303042 | -0.139125 | -0.127823 | -0.239652 | -0.115844 | -0.495076 | -0.086215 | -0.126064 | -0.002767 | -0.001738 | 0.01267 | -0.020334 | 0.009344 | -0.004233 | 0.01963 | 0.00595 | . 890 -0.137239 | 0.673301 | 0.772035 | -0.261841 | -0.357309 | -0.024792 | -0.031719 | -0.060202 | -0.066855 | -0.163837 | -0.032854 | -0.075613 | -0.024524 | -0.026264 | 0.0 | -0.018956 | -0.29693 | -0.171430 | -0.273011 | -0.166394 | -0.067034 | -0.033377 | -0.018956 | 0.0 | 0.441685 | 0.366176 | -0.636856 | -0.194551 | 0.053991 | -1.138351 | -0.834174 | -0.043653 | -0.118211 | -0.128788 | -0.182533 | -0.12460 | -0.126178 | 0.330408 | -0.558287 | -0.159665 | -0.119665 | -0.500465 | -0.485321 | -0.691966 | -0.021922 | -0.085250 | -0.012693 | -0.001738 | 0.01267 | -0.008261 | 0.007704 | -0.012410 | 0.01963 | 0.00595 | . pca = decomposition.PCA(n_components=2) pca.fit(embeded_xs) X = pca.transform(embeded_xs) . y = learn.dls.targ X[:4] . array([[-0.61446041, 0.41986648], [-0.61700676, 0.98989824], [ 2.33902372, -2.63107592], [-0.75357453, 1.18865188]]) . plt.scatter(X[:, 0], X[:, 1], c=y.values) # this is pca without embeddings . &lt;matplotlib.collections.PathCollection at 0x7f026431d940&gt; . import plotly.express as px . df.columns . Index([&#39;name&#39;, &#39;market&#39;, &#39;funding_total_usd&#39;, &#39;status&#39;, &#39;country_code&#39;, &#39;state_code&#39;, &#39;region&#39;, &#39;city&#39;, &#39;funding_rounds&#39;, &#39;founded_month&#39;, &#39;founded_quarter&#39;, &#39;founded_year&#39;, &#39;seed&#39;, &#39;venture&#39;, &#39;equity_crowdfunding&#39;, &#39;undisclosed&#39;, &#39;convertible_note&#39;, &#39;debt_financing&#39;, &#39;angel&#39;, &#39;grant&#39;, &#39;private_equity&#39;, &#39;post_ipo_equity&#39;, &#39;post_ipo_debt&#39;, &#39;secondary_market&#39;, &#39;product_crowdfunding&#39;, &#39;round_A&#39;, &#39;round_B&#39;, &#39;round_C&#39;, &#39;round_D&#39;, &#39;round_E&#39;, &#39;round_F&#39;, &#39;round_G&#39;, &#39;round_H&#39;, &#39;first_funding_year&#39;, &#39;last_funding_year&#39;, &#39;delta_first_fund_founded&#39;, &#39;ave_rounds_per_year&#39;, &#39;delta_seed_to_Round_A&#39;, &#39;mean_fund_for_market&#39;, &#39;median_fund_for_market&#39;, &#39;delta_total_fund_mean_for_market&#39;, &#39;delta_total_fund_median_for_market&#39;, &#39;mean_fund_for_city&#39;, &#39;median_fund_for_city&#39;, &#39;delta_total_fund_mean_for_city&#39;, &#39;delta_total_fund_median_for_city&#39;, &#39;mean_fund_for_first_funding_year&#39;, &#39;median_fund_for_first_funding_year&#39;, &#39;delta_total_fund_mean_for_first_funding_year&#39;, &#39;delta_total_fund_median_for_first_funding_year&#39;, &#39;mean_fund_for_market_and_first_funding_year&#39;, &#39;median_fund_for_market_and_first_funding_year&#39;, &#39;count_fund_for_market_and_first_funding_year&#39;, &#39;delta_total_fund_mean_for_market_and_first_funding_year&#39;, &#39;delta_total_fund_median_for_market_and_first_funding_year&#39;], dtype=&#39;object&#39;) . fig = px.scatter(x=X[:, 0], y=X[:, 1], color=y.values, marginal_y=&quot;violin&quot;, text=df_out.name, marginal_x=&quot;box&quot;, trendline=&quot;ols&quot;) . fig = px.scatter(x=X[:, 0], y=X[:, 1], color=y.values, marginal_y=&quot;violin&quot;, text=df_out2.city, marginal_x=&quot;box&quot;, trendline=&quot;ols&quot;) . fig.show() . . . df.names . AttributeError Traceback (most recent call last) &lt;ipython-input-89-efaf84cedc8a&gt; in &lt;module&gt;() -&gt; 1 df.names /usr/local/lib/python3.6/dist-packages/pandas/core/generic.py in __getattr__(self, name) 5139 if self._info_axis._can_hold_identifiers_and_holds_name(name): 5140 return self[name] -&gt; 5141 return object.__getattribute__(self, name) 5142 5143 def __setattr__(self, name: str, value) -&gt; None: AttributeError: &#39;DataFrame&#39; object has no attribute &#39;names&#39; . from sklearn.manifold import TSNE tsne = TSNE() X = tsne.fit_transform(embeded_xs) . from sklearn.manifold import TSNE tsne = TSNE(perplexity=3.0, early_exaggeration=12.0, learning_rate=200.0, n_iter=1000) X = tsne.fit_transform(d) . # marginal_x=&quot;box&quot;, trendline=&quot;ols&quot;) fig = px.scatter(x=X_embedded[:, 0], y=X_embedded[:, 1], color=y.values, text=df_out2.city) fig.show() . . . fig = px.scatter(x=X_embedded[:, 0], y=X_embedded[:, 1], color=y.values) fig.show() . . . import seaborn as sns sns.set(rc={&#39;figure.figsize&#39;:(11.7,8.27)}) palette = sns.color_palette(&quot;bright&quot;, 10) sns.scatterplot(data=X_embedded, x=X_embedded[:,0], y=X_embedded[:,1], hue=learn.dls.targ.status) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f027574beb8&gt; . combine nn, rf+nn_ebeddings, xgb+nn_ebeddings . # clf = RandomForestClassifier(max_depth=200,random_state=0, oob_score=True, min_samples_split=30) #learn = tabular_learner(dls,layers=[2000,1000],metrics=accuracy , emb_szs={&#39;city&#39;:6, &#39;market&#39;:4}) es=4 learn = tabular_learner(dls,layers=[2000,1000],metrics=accuracy , emb_szs={&#39;city&#39;:es, &#39;market&#39;:es, &#39;region&#39;:es, &#39;founded_month&#39;:es, &#39;founded_quarter&#39;:es, &#39;state_code&#39;:es}) learn.fit_one_cycle(20, .0001, cbs=EarlyStoppingCallback(monitor=&#39;accuracy&#39;, min_delta=0.1, patience=10)) # clf.fit(embeded_xs, learn.dls.targ) # after fitting, get embeddings to be used by ML embeded_xs = embed_features(learn, learn.dls.train.xs) xs_valid = embed_features(learn, learn.dls.valid.xs) . epoch train_loss valid_loss accuracy time . 0 | 0.712309 | 0.687893 | 0.556035 | 00:01 | . 1 | 0.698926 | 0.670376 | 0.626437 | 00:01 | . 2 | 0.680011 | 0.643916 | 0.650862 | 00:01 | . 3 | 0.662179 | 0.631757 | 0.643678 | 00:01 | . 4 | 0.645592 | 0.625145 | 0.659483 | 00:01 | . 5 | 0.630624 | 0.621407 | 0.643678 | 00:01 | . 6 | 0.616132 | 0.625613 | 0.643678 | 00:01 | . 7 | 0.601899 | 0.610999 | 0.675287 | 00:01 | . 8 | 0.588286 | 0.618205 | 0.655172 | 00:01 | . 9 | 0.576180 | 0.626794 | 0.649425 | 00:01 | . 10 | 0.564495 | 0.621316 | 0.658046 | 00:01 | . 11 | 0.553813 | 0.621556 | 0.663793 | 00:01 | . 12 | 0.543681 | 0.622380 | 0.668103 | 00:01 | . 13 | 0.533751 | 0.624668 | 0.660920 | 00:01 | . 14 | 0.525027 | 0.619503 | 0.672414 | 00:01 | . No improvement since epoch 4: early stopping . _, embeded_xs, xs_valid = nn_embeddings() . epoch train_loss valid_loss accuracy time . 0 | 0.703206 | 0.688736 | 0.570402 | 00:01 | . 1 | 0.688570 | 0.669758 | 0.626437 | 00:01 | . 2 | 0.669762 | 0.643662 | 0.649425 | 00:01 | . 3 | 0.652722 | 0.626828 | 0.666667 | 00:01 | . 4 | 0.637052 | 0.619145 | 0.666667 | 00:01 | . 5 | 0.622569 | 0.616981 | 0.675287 | 00:01 | . 6 | 0.609276 | 0.609329 | 0.672414 | 00:01 | . 7 | 0.595564 | 0.611797 | 0.658046 | 00:01 | . 8 | 0.583293 | 0.612067 | 0.662356 | 00:01 | . 9 | 0.570897 | 0.607964 | 0.669540 | 00:01 | . 10 | 0.559372 | 0.613921 | 0.660920 | 00:01 | . 11 | 0.548942 | 0.614148 | 0.659483 | 00:01 | . 12 | 0.539154 | 0.614083 | 0.655172 | 00:01 | . 13 | 0.530141 | 0.615041 | 0.659483 | 00:01 | . 14 | 0.521409 | 0.612653 | 0.653736 | 00:01 | . 15 | 0.513907 | 0.610913 | 0.655172 | 00:01 | . No improvement since epoch 5: early stopping . clf = RandomForestClassifier(max_depth=50, random_state=0, oob_score=False, min_samples_split=50) clf.fit(embeded_xs, learn.dls.targ.values.ravel()) clf.score(xs_valid, y_test) . 0.7212643678160919 . len(xs_valid), len(y_test) . (696, 696) . learn, embeded_xs, xs_valid = nn_embeddings() # learn, embeded_xs, xs_valid = nn_embeddings(city_es = 1, market_es = 1, region_es = 1, # founded_month_es = 1, founded_quarter_es = 1, # state_code_es = 1) clf = RandomForestClassifier(max_depth=50, random_state=0, oob_score=False, min_samples_split=50) clf.fit(embeded_xs, learn.dls.targ.values.ravel()) clf.score(xs_valid, y_test) . 0.0009120108559727668 0.002511886414140463 . epoch train_loss valid_loss accuracy time . 0 | 0.668907 | 0.649800 | 0.640805 | 00:01 | . 1 | 0.640508 | 0.643623 | 0.650862 | 00:01 | . 2 | 0.604518 | 0.645920 | 0.662356 | 00:01 | . No improvement since epoch 0: early stopping . 0.7140804597701149 . nn_preds,targs = learn.get_preds() #this is validaation predictions pnn = pd.DataFrame(nn_preds,columns=[&#39;nn_0&#39;,&#39;nn_1&#39;]).astype(&quot;float&quot;) . model_embed = XGBClassifier(max_depth=3,n_estimators=100,learning_rate=.01) model_embed.fit(embeded_xs, learn.dls.targ.values.ravel()) print (model_embed.score(xs_valid, y_test)) probabilities_xgb=model_embed.predict_proba(xs_valid) #probabilities_xgb=model_embed.predict_log_proba(xs_valid) pxgb = pd.DataFrame(probabilities_xgb,columns=[&#39;xgb_0&#39;,&#39;xgb_1&#39;]).astype(&quot;float&quot;) . 0.7097701149425287 . pxgb[:4] . xgb_0 xgb_1 . 0 0.458098 | 0.541902 | . 1 0.425292 | 0.574708 | . 2 0.419124 | 0.580876 | . 3 0.391148 | 0.608852 | . clf = RandomForestClassifier(max_depth=50, random_state=0, oob_score=False, min_samples_split=50) clf.fit(embeded_xs, learn.dls.targ.values.ravel()) print (clf.score(xs_valid, y_test)) #rf_prob = clf.predict_proba(xs_valid) rf_prob = clf.predict_log_proba(xs_valid) prf = pd.DataFrame(rf_prob, columns=[&#39;rf_0&#39;,&#39;rf_1&#39;]).astype(&quot;float&quot;) prf[:4] . 0.7140804597701149 . rf_0 rf_1 . 0 -0.679384 | -0.707102 | . 1 -1.010366 | -0.452691 | . 2 -1.188892 | -0.363209 | . 3 -1.426062 | -0.274770 | . Combine all models into a dataframe . result = pd.concat([validation_company_names, pnn, prf_prob_no_ebedding, prf, pxgb, pxgb_prob_no_ebedding ], axis=1, join=&quot;inner&quot;) result[&#39;nn&#39;] = (result.nn_1 - result.nn_0) result[&#39;xgb&#39;] = (result.xgb_1 - result.xgb_0) result[&#39;rf&#39;] = (result.rf_1 - result.rf_0) result[&#39;rf_no_embed&#39;] = result.rf_1_no_ebedding - result.rf_0_no_ebedding result[&#39;xgb_no_embed&#39;] = result.xgb_1_no_ebedding - result.xgb_0_no_ebedding result[&#39;confidence_total&#39;] = (result.nn_1 - result.nn_0) + (result.xgb_1 - result.xgb_0) + (result.rf_1 - result.rf_0)+(result.rf_1_no_ebedding - result.rf_0_no_ebedding) + (result.xgb_1_no_ebedding - result.xgb_0_no_ebedding) result[:4] . name nn_0 nn_1 rf_0_no_ebedding rf_1_no_ebedding rf_0 rf_1 xgb_0 xgb_1 xgb_0_no_ebedding xgb_1_no_ebedding nn xgb rf rf_no_embed xgb_no_embed confidence_total . 0 Hyperpublic | 0.372120 | 0.627880 | -0.715404 | -0.671375 | -0.679384 | -0.707102 | 0.458098 | 0.541902 | -0.715404 | -0.671375 | 0.255760 | 0.083803 | -0.027718 | 0.044029 | 0.044029 | 0.399904 | . 1 Syncplicity | 0.354694 | 0.645306 | -0.885880 | -0.531626 | -1.010366 | -0.452691 | 0.425292 | 0.574708 | -0.885880 | -0.531626 | 0.290612 | 0.149416 | 0.557675 | 0.354254 | 0.354254 | 1.706209 | . 2 Photobucket | 0.127760 | 0.872240 | -1.170952 | -0.371168 | -1.188892 | -0.363209 | 0.419124 | 0.580876 | -1.170952 | -0.371168 | 0.744479 | 0.161753 | 0.825683 | 0.799784 | 0.799784 | 3.331483 | . 3 StartForce | 0.209060 | 0.790940 | -1.097743 | -0.405900 | -1.426062 | -0.274770 | 0.391148 | 0.608852 | -1.097743 | -0.405900 | 0.581880 | 0.217705 | 1.151292 | 0.691843 | 0.691843 | 3.334562 | . X=result.copy() X.set_index(&#39;name&#39;,inplace=True) y=targs seed = 7 test_size = 0.2 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed) . clf = RandomForestClassifier(max_depth=7, random_state=0, oob_score=False, min_samples_split=100,n_estimators=50) clf.fit(X_train, y_train) #clf.score(X_train, y_train) clf.score(X_test, y_test) . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). . 0.7785714285714286 . clf = RandomForestClassifier(max_depth=3, random_state=0, oob_score=False, min_samples_split=100, n_estimators=5) clf.fit(X_train, y_train) #clf.score(X_train, y_train) print (clf.score(X_test, y_test)) predicted=clf.predict(X_test) . 0.7642857142857142 . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). . fi = rf_feat_importance(clf, X_test) plot_fi(fi[:30]); . fi[:3] . cols imp . 13 rf_no_embed | 0.286066 | . 15 confidence_total | 0.174468 | . 11 xgb | 0.173207 | . to_keep = fi[fi.imp&gt;0.02].cols print (len(fi.cols), len(to_keep)) xs_df = X_train[to_keep] valid_xs_df = X_test[to_keep] . 16 7 . clf = RandomForestClassifier(max_depth=3, random_state=0, oob_score=False, min_samples_split=100, n_estimators=5) clf.fit(xs_df, y_train) #clf.score(X_train, y_train) print (clf.score(valid_xs_df, y_test)) predicted=clf.predict(valid_xs_df) . 0.7785714285714286 . /usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). . # model.fit(X_train, y_train) # model, model.score(X_train, y_train), model.score(X_test, y_test) . # clf.fit(X_train, y_train) # clf.score(X_train, y_train), clf.score(X_test, y_test) . import numpy as np from sklearn.metrics import plot_confusion_matrix from sklearn.metrics import confusion_matrix def plot_confusion_matrix(cm, target_names, title=&#39;Confusion matrix&#39;, cmap=None, normalize=True): &quot;&quot;&quot; given a sklearn confusion matrix (cm), make a nice plot Arguments cm: confusion matrix from sklearn.metrics.confusion_matrix target_names: given classification classes such as [0, 1, 2] the class names, for example: [&#39;high&#39;, &#39;medium&#39;, &#39;low&#39;] title: the text to display at the top of the matrix cmap: the gradient of the values displayed from matplotlib.pyplot.cm see http://matplotlib.org/examples/color/colormaps_reference.html plt.get_cmap(&#39;jet&#39;) or plt.cm.Blues normalize: If False, plot the raw numbers If True, plot the proportions Usage -- plot_confusion_matrix(cm = cm, # confusion matrix created by # sklearn.metrics.confusion_matrix normalize = True, # show proportions target_names = y_labels_vals, # list of names of the classes title = best_estimator_name) # title of graph Citiation http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html &quot;&quot;&quot; import matplotlib.pyplot as plt import numpy as np import itertools accuracy = np.trace(cm) / float(np.sum(cm)) misclass = 1 - accuracy if cmap is None: cmap = plt.get_cmap(&#39;Blues&#39;) plt.figure(figsize=(8, 6)) plt.imshow(cm, interpolation=&#39;nearest&#39;, cmap=cmap) plt.title(title) plt.colorbar() if target_names is not None: tick_marks = np.arange(len(target_names)) plt.xticks(tick_marks, target_names, rotation=45) plt.yticks(tick_marks, target_names) if normalize: cm = cm.astype(&#39;float&#39;) / cm.sum(axis=1)[:, np.newaxis] thresh = cm.max() / 1.5 if normalize else cm.max() / 2 for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): if normalize: plt.text(j, i, &quot;{:0.4f}&quot;.format(cm[i, j]), horizontalalignment=&quot;center&quot;, color=&quot;white&quot; if cm[i, j] &gt; thresh else &quot;black&quot;) else: plt.text(j, i, &quot;{:,}&quot;.format(cm[i, j]), horizontalalignment=&quot;center&quot;, color=&quot;white&quot; if cm[i, j] &gt; thresh else &quot;black&quot;) plt.tight_layout() plt.ylabel(&#39;True label&#39;) plt.xlabel(&#39;Predicted label naccuracy={:0.4f}; misclass={:0.4f}&#39;.format(accuracy, misclass)) plt.show() . cm=confusion_matrix(y_test, predicted, labels=None, sample_weight=None, normalize=None) plot_confusion_matrix(cm, [&#39;closed&#39;,&#39;aquired&#39;], normalize=None) . NameError Traceback (most recent call last) &lt;ipython-input-106-116efa171441&gt; in &lt;module&gt;() -&gt; 1 cm=confusion_matrix(y_test, predicted, labels=None, sample_weight=None, normalize=None) 2 plot_confusion_matrix(cm, [&#39;closed&#39;,&#39;aquired&#39;], normalize=None) NameError: name &#39;predicted&#39; is not defined . Let&#39;s only pick confident values for matrix . predicted_val=clf.predict_log_proba(valid_xs_df) predicted=clf.predict(valid_xs_df) . X_test.index[:2] . Index([&#39;Social GameWorks&#39;, &#39;Gifts that Give&#39;], dtype=&#39;object&#39;, name=&#39;name&#39;) . def get_max(x): if x[&#39;diff&#39;]&gt;0: x.p=1 else: x.p=0 return x pred_df=pd.DataFrame(predicted_val, columns=[&#39;z&#39;,&#39;o&#39;]) pred_df[&#39;names&#39;]=(X_test.index) pred_df[&#39;tar&#39;]=y_test.numpy() pred_df[&#39;p&#39;]=y_test.numpy() pred_df[&#39;pred&#39;]=predicted pred_df[&#39;diff&#39;] = pred_df.apply(lambda x: x[&#39;o&#39;] - x[&#39;z&#39;], axis=1) pred_df=pred_df.apply(get_max, axis=1) pred_df=pred_df[np.abs(pred_df[&#39;diff&#39;])&gt;1.4] . pred_df.sort_values(by=&#39;diff&#39;)[:60] . z o names tar p pred diff . 0 -0.168510 | -1.863833 | Social GameWorks | 0 | 0 | 0 | -1.695324 | . 133 -0.168510 | -1.863833 | Posiq | 0 | 0 | 0 | -1.695324 | . 123 -0.168510 | -1.863833 | MobAppCreator | 0 | 0 | 0 | -1.695324 | . 93 -0.168510 | -1.863833 | Ground Zero Group Corporation | 0 | 0 | 0 | -1.695324 | . 89 -0.168510 | -1.863833 | Your Survival | 0 | 0 | 0 | -1.695324 | . 82 -0.168510 | -1.863833 | Timbre | 1 | 0 | 0 | -1.695324 | . 81 -0.168510 | -1.863833 | SMR SITE | 0 | 0 | 0 | -1.695324 | . 78 -0.168510 | -1.863833 | SensiGen | 1 | 0 | 0 | -1.695324 | . 73 -0.168510 | -1.863833 | Floop | 0 | 0 | 0 | -1.695324 | . 66 -0.168510 | -1.863833 | Frugalo | 1 | 0 | 0 | -1.695324 | . 135 -0.168510 | -1.863833 | MyDatingTree | 0 | 0 | 0 | -1.695324 | . 52 -0.168510 | -1.863833 | Puentes Company | 0 | 0 | 0 | -1.695324 | . 34 -0.168510 | -1.863833 | VirtualQube | 0 | 0 | 0 | -1.695324 | . 137 -0.168510 | -1.863833 | FirePower Technology | 0 | 0 | 0 | -1.695324 | . 9 -0.168510 | -1.863833 | JumpTheClub | 0 | 0 | 0 | -1.695324 | . 1 -0.168510 | -1.863833 | Gifts that Give | 0 | 0 | 0 | -1.695324 | . 26 -0.168510 | -1.863833 | Cerelink | 0 | 0 | 0 | -1.695324 | . 2 -0.168510 | -1.863833 | WHObyYOU | 0 | 0 | 0 | -1.695324 | . 19 -0.168510 | -1.863833 | Dormzy | 0 | 0 | 0 | -1.695324 | . 3 -0.168510 | -1.863833 | Khush | 1 | 0 | 0 | -1.695324 | . 10 -0.168510 | -1.863833 | Keepio | 0 | 0 | 0 | -1.695324 | . 33 -0.168510 | -1.863833 | MediProPharma | 0 | 0 | 0 | -1.695324 | . 128 -1.674355 | -0.207552 | Fastpoint Games | 1 | 1 | 1 | 1.466803 | . 118 -1.674355 | -0.207552 | Melodeo | 1 | 1 | 1 | 1.466803 | . 104 -1.817653 | -0.177222 | AdReady | 1 | 1 | 1 | 1.640430 | . 95 -1.817653 | -0.177222 | Simple | 1 | 1 | 1 | 1.640430 | . 5 -1.817653 | -0.177222 | Where | 1 | 1 | 1 | 1.640430 | . 29 -1.817653 | -0.177222 | Crossing Automation | 1 | 1 | 1 | 1.640430 | . 80 -1.817653 | -0.177222 | Formspring | 1 | 1 | 1 | 1.640430 | . 20 -1.817653 | -0.177222 | iLike | 1 | 1 | 1 | 1.640430 | . 63 -1.817653 | -0.177222 | Breaktime Studios | 0 | 1 | 1 | 1.640430 | . 23 -1.817653 | -0.177222 | Sendia | 1 | 1 | 1 | 1.640430 | . 43 -1.817653 | -0.177222 | Pivot | 1 | 1 | 1 | 1.640430 | . 41 -1.817653 | -0.177222 | Kickfire | 0 | 1 | 1 | 1.640430 | . 87 -1.817653 | -0.177222 | Jumptap | 1 | 1 | 1 | 1.640430 | . 53 -1.817653 | -0.177222 | ConSentry Networks | 0 | 1 | 1 | 1.640430 | . 85 -2.289229 | -0.106856 | SpectraLinear | 1 | 1 | 1 | 2.182373 | . 17 -2.289229 | -0.106856 | fabrik | 1 | 1 | 1 | 2.182373 | . cm=confusion_matrix(pred_df.tar, pred_df.p, labels=None, sample_weight=None, normalize=None) plot_confusion_matrix(cm, [&#39;closed&#39;,&#39;aquired&#39;], normalize=None) . pred_df[:6] . Load investment data and compare to original. . If there is overlap, we can combine . base_dir = root_dir + &#39;crunchbase/master&#39; path = Path(base_dir) csv_path = Path(base_dir+&#39;/investments.csv&#39;) investments_df=pd.read_csv(csv_path) . investments_df.columns . Index([&#39;company_permalink&#39;, &#39;company_name&#39;, &#39;company_category_list&#39;, &#39;company_country_code&#39;, &#39;company_state_code&#39;, &#39;company_region&#39;, &#39;company_city&#39;, &#39;investor_permalink&#39;, &#39;investor_name&#39;, &#39;investor_country_code&#39;, &#39;investor_state_code&#39;, &#39;investor_region&#39;, &#39;investor_city&#39;, &#39;funding_round_permalink&#39;, &#39;funding_round_type&#39;, &#39;funding_round_code&#39;, &#39;funded_at&#39;, &#39;raised_amount_usd&#39;], dtype=&#39;object&#39;) . investments_df.company_name.describe() . count 168646 unique 44578 top Uber freq 64 Name: company_name, dtype: object . i_comp=set(investments_df.company_name.unique()) . len(i_comp) . 44579 . d_cop=set(df.name.unique()) . len(d_cop) . 3480 . z = d_cop.intersection(i_comp) . len(z) . 2611 . investments_df.investor_permalink.fillna(&#39;xxx&#39;, inplace=True) investments_df = investments_df[investments_df.company_country_code==&#39;USA&#39;] unique_investors=investments_df.copy() unique_investors = investments_df[[&#39;company_name&#39;,&#39;investor_permalink&#39;,&#39;investor_name&#39;,&#39;funded_at&#39;]].copy() . unique_investors[:5] . company_name investor_permalink investor_name funded_at . 1 004 Technologies | /organization/venturecapital-de | VCDE Venture Partners | 2014-07-24 | . 3 H2O.ai | /organization/capital-one | Capital One | 2015-11-09 | . 4 H2O.ai | /organization/nexus-venture-partners | Nexus Venture Partners | 2013-05-22 | . 5 H2O.ai | /organization/nexus-venture-partners | Nexus Venture Partners | 2015-11-09 | . 6 H2O.ai | /organization/nexus-venture-partners | Nexus Venture Partners | 2013-01-03 | . unique_investors.investor_permalink=unique_investors.investor_permalink.astype(&#39;string&#39;) #unique_investors.investor_permalink=unique_investors.investor_permalink.to_string() . n=unique_investors.investor_permalink.str.split(&#39;/&#39;,n =2, expand = True) n . 0 1 2 . 1 | organization | venturecapital-de | . 3 | organization | capital-one | . 4 | organization | nexus-venture-partners | . 5 | organization | nexus-venture-partners | . 6 | organization | nexus-venture-partners | . ... ... | ... | ... | . 168630 | organization | prolog-ventures | . 168631 | organization | state-of-wisconsin-investment-board | . 168632 | organization | stonehenge-capital | . 168633 | organization | venture-investors | . 168645 | organization | startupbootcamp | . 112283 rows × 3 columns . unique_investors[&#39;t&#39;]=n[1] . TypeError Traceback (most recent call last) &lt;ipython-input-59-e4ac416b35ce&gt; in &lt;module&gt;() -&gt; 1 unique_investors[&#39;t&#39;]=n[1].values() TypeError: &#39;numpy.ndarray&#39; object is not callable . new_df.columns . RangeIndex(start=0, stop=3, step=1) . #unique_investors.investor_permalink.fillna(&#39;xxx&#39;, inplace=True) #unique_investors.investor_permalink=unique_investors.investor_permalink.to_string() unique_investors.loc[unique_investors[&#39;investor_permalink&#39;].str.contains(&quot;organization&quot;),&#39;investor_type&#39;] = &#39;organization&#39; unique_investors.loc[unique_investors[&#39;investor_permalink&#39;].str.contains(&quot;person&quot;),&#39;investor_type&#39;] = &#39;person&#39; . unique_investors=unique_investors.drop_duplicates() unique_investors.shape . (112065, 3) . unique_investors[[&#39;investor_name&#39;,&#39;p&#39;]].nunique() . investor_name 19687 p 3 dtype: int64 . how many VC companies . org_invest=unique_investors[unique_investors.p==&#39;organization&#39;] org_invest.groupby(by=[&#39;company_name&#39;,&#39;funded_at&#39;])[&#39;investor_name&#39;].count().agg([&quot;mean&quot;, &quot;median&quot;, &quot;max&quot;, &quot;min&quot;]) . mean 2.407946 median 2.000000 max 22.000000 min 1.000000 Name: investor_name, dtype: float64 . org_invest.investor_name.describe() . count 93333 unique 11408 top New Enterprise Associates freq 810 Name: investor_name, dtype: object . unique_investors.groupby(by=[&#39;company_name&#39;])[&#39;investor_name&#39;].count() . company_name #BratPackStyle, LLC. 1 #waywire 6 . 1 //Staq.io 1 004 Technologies 1 .. zkipster 1 zlien 3 zulily 6 zuuka! 5 Ôasys 1 Name: investor_name, Length: 23040, dtype: int64 . t=unique_investors.groupby(by=[&#39;company_name&#39;,&#39;funded_at&#39;])[&#39;investor_name&#39;] t . &lt;pandas.core.groupby.generic.SeriesGroupBy object at 0x7fa5612a1908&gt; . unique_investors[&#39;total_unique_img_rows_per_patient&#39;]=unique_investors.groupby([&#39;company_name&#39;,&#39;funded_at&#39;])[&quot;investor_name&quot;].transform(&#39;nunique&#39;) . def get_investor_type(df): #if df[&#39;investor_permalink&#39;].contains(&#39;organization&#39;): if &#39;organization&#39; in df[&#39;investor_permalink&#39;]: df[&#39;p&#39;]=&#39;organization&#39; elif &#39;person&#39; in df[&#39;investor_permalink&#39;]: # elif df.investor_permalink.str.contains(&#39;person&#39;): df[&#39;p&#39;]=&#39;person&#39; else: df[&#39;p&#39;]=&#39;xxx&#39; return df unique_investors=unique_investors.apply(get_investor_type, axis=1) . unique_investors[&#39;investor_permalink&#39;][:20] . 1 /organization/venturecapital-de 3 /organization/capital-one 4 /organization/nexus-venture-partners 5 /organization/nexus-venture-partners 6 /organization/nexus-venture-partners 7 /organization/nexus-venture-partners 8 /organization/paxion-capital-partners 9 /organization/transamerica 10 /organization/transamerica-ventures 11 /person/anand-babu-periasamy 12 /person/anand-rajaraman-2 13 /person/ash-bhardwaj 14 /person/michael-marks 15 /person/rajesh-ambati 16 /person/rakesh-mathur 17 /organization/camp-one-ventures 18 /organization/camp-one-ventures 19 /organization/charles-river-ventures 20 /organization/general-catalyst-partners 21 /organization/kima-ventures Name: investor_permalink, dtype: object . unique_investors.p.value_counts() . organization 93333 person 18943 xxx 7 Name: p, dtype: int64 .",
            "url": "https://patrick-hanley.github.io/thought_experiments/2021/02/05/fastai_crunchbase.html",
            "relUrl": "/2021/02/05/fastai_crunchbase.html",
            "date": " • Feb 5, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://patrick-hanley.github.io/thought_experiments/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "ghtop redux",
            "content": ". Introduction . We recently refactored the CLI tool ghtop, created by the CEO of GitHub, Nat Friedman. Nat even described our refactor as a “tour de force”. This post describes what we learned along the way. . Motivation . Recently, we released ghapi, a new python client for the GitHub API. ghapi provides unparalleled ease of access to the GitHub api, as well as utilities for interacting with GitHub Actions. Part of our motivation for creating ghapi was to accelerate the development of build, testing and deployment tools that help us in maintaining fastai projects. . We recently started using GitHub Actions to perform a wide variety of tasks automatically like: unit and integration tests, deploying documentation, building Docker containers and Conda packages, sharing releases on Twitter, and much more. This automation is key to maintaining the vast open source fastai ecosystem with very few maintainers. . Since ghapi is central to so many of these tasks, we wanted to stress-test its efficacy against other projects. That’s when we found ghtop. This tool allows you to stream all the public events happening on GitHub to a CLI dashboard. We thought it would be a fun learning experience to refactor this code base with various fastai tools such as ghapi and fastcore, but also try out new libraries like rich. . Features we added to our tools . While exploring ghtop, we added several features to various fastai tools that we found to be generally useful. . ghapi Authentication . We added the function github_auth_device which allows users to authenticate their api client with GitHub interactively in a browser. When we call this function we get the following prompt: . github_auth_device() . First copy your one-time code: 276E-C910 Then visit https://github.com/login/device in your browser, and paste the code when prompted. Shall we try to open the link for you? [y/n] . The browser opens a window that looks like this: . . The function then returns an authenticated token which you can use for various tasks. While this is not the only way to create a token, this is a user friendly way to create a token, especially for those who are not as familiar with GitHub. . ghapi Events . As a result of our explorations with ghtop, we added an event module to ghapi. This is useful for retrieving and inspecting sample events. Inspecting sample events is important as it allows you to prototype GitHub Actions workflows locally. You can sample real events with load_sample_events: . from ghapi.event import load_sample_events evts = load_sample_events() . Individual events are formatted as markdown lists to be human readable in Jupyter: . print(evts[0]) . - id: 14517925737 - type: PushEvent - actor: - id: 17030246 - login: BeckhamL - display_login: BeckhamL - gravatar_id: - url: https://api.github.com/users/BeckhamL - avatar_url: https://avatars.githubusercontent.com/u/17030246? - repo: - id: 154349747 - name: BeckhamL/leetcode - url: https://api.github.com/repos/BeckhamL/leetcode - payload: - push_id: 6194986903 - size: 1 - distinct_size: 1 - ref: refs/heads/master - head: 2055b0fcf22f1c3543e38b60199f6882266d32a5 - before: cb16921949c969b5153a0c23ce8fe516d2c8d773 - commits: - - sha: 2055b0fcf22f1c3543e38b60199f6882266d32a5 - author: - email: beckham.lam@mail.mcgill.ca - name: Beckham Lam - message: Create detectCapital.ts - distinct: True - url: https://api.github.com/repos/BeckhamL/leetcode/commits/2055b0fcf22f1c3543e38b60199f6882266d32a5 - public: True - created_at: 2020-12-13T21:32:34Z . You can also inspect the json data in an event, which are accessible as attributes: . evts[0].type . &#39;PushEvent&#39; . For example, here is the frequency of all full_types in the sample: . x,y = zip(*Counter([o.full_type for o in evts]).most_common()) plt.figure(figsize=(8, 6)) plt.barh(x[::-1],y[::-1]); . We can fetch public events in parallel with GhApi.list_events_parallel. In our experiments, repeatedly calling list_events_parallel is fast enough to fetch all current public activity from all users across the entire GitHub platform. We use this for ghtop. Behind the scenes, list_events_parallel uses Python&#39;s ThreadPoolExecutor to fetch events in parallel - no fancy distributed systems or complicated infrastructure necessary, even at the scale of GitHub! . %time api = GhApi() evts = api.list_events_parallel() len(evts) . CPU times: user 2 µs, sys: 0 ns, total: 2 µs Wall time: 4.29 µs . 240 . Note that the GitHub API is stateless, so successive calls to the API will likely return events already seen. We handle this by using a set operations to filter out events already seen. . ghapi pagination . One of the most cumbersome aspects of fetching lots of data from the GitHub api can be pagination. As mentioned in the documentation, different endpoints have different pagination rules and defaults. Therefore, many api clients offer clunky or incomplete interfaces for pagination. . In ghapi we added an entire module with various tools to make paging easier. Below is an example for retrieving repos for the github org. Without pagination, we can only retrieve a fixed number at a time (by default 30): . api = GhApi() repos = api.repos.list_for_org(&#39;fastai&#39;) len(repos) . 30 . However, to get more we can paginate through paged: . from ghapi.event import paged repos = paged(api.repos.list_for_org, &#39;fastai&#39;) for page in repos: print(len(page), page[0].name) . 30 fast-image 30 fastforest 30 .github 8 tweetrel . You can learn more about this functionality by reading the docs. . fastcore Sparklines . Part of goals for refactoring ghtop were to introduce cool visualizations in the terminal of data. We drew inspiration from projects like bashtop, which have CLI interfaces that look like this: . Concretely, we really liked the idea of sparklines in the terminal. Therefore, we created the ability to show sparklines with fastcore: . from fastcore.utils import sparkline data = [9,6,None,1,4,0,8,15,10] print(f&#39;without &quot;empty_zero&quot;: {sparkline(data, empty_zero=False)}&#39;) print(f&#39; with &quot;empty_zero&quot;: {sparkline(data, empty_zero=True )}&#39;) . without &#34;empty_zero&#34;: ▅▂ ▁▂▁▃▇▅ with &#34;empty_zero&#34;: ▅▂ ▁▂ ▃▇▅ . For more information on this function, read the docs. Later in this post, we will describe how we used Rich to add color and animation to these sparklines. . fastcore EventTimer . Because we wanted streaming event data to automatically populate sparklines, we created EventTimer that constructs a histogram according to a frequency and time span you set. With EventTimer, you can add events with add, and get the number of events and their frequency: . from fastcore.utils import EventTimer from time import sleep import random def _randwait(): yield from (sleep(random.random()/200) for _ in range(100)) c = EventTimer(store=5, span=0.03) for o in _randwait(): c.add(1) print(f&#39;Num Events: {c.events}, Freq/sec: {c.freq:.01f}&#39;) print(&#39;Most recent: &#39;, sparkline(c.hist), *L(c.hist).map(&#39;{:.01f}&#39;)) . Num Events: 6, Freq/sec: 301.1 Most recent: ▃▁▁▇▁ 323.6 274.8 291.3 390.9 283.6 . For more information, see the docs. . CLI Animations With Rich . Rich is an amazing python library that allows you to create beautiful, animated and interactive CLI interfaces. Below is a preview of some its features: . Rich also offers animated elements like spinners: . ... and progress bars: . While this post is not about rich, we highly recommend visiting the repo and the docs to learn more. Rich allows you to create your own custom elements. We created two custom elements - Stats and FixedPanel, which we describe below: . Stats: Sparklines with metrics . Stats renders a group of sparklines along with a spinner and a progress bar. First we define our sparklines, the last argument being a list of event types to count: . from ghtop.richext import * from ghtop.all_rich import * console = Console() s1 = ESpark(&#39;Issues&#39;, &#39;green&#39;, [IssueCommentEvent, IssuesEvent]) s2 = ESpark(&#39;PR&#39;, &#39;red&#39;, [PullRequestEvent, PullRequestReviewCommentEvent, PullRequestReviewEvent]) s3 = ESpark(&#39;Follow&#39;, &#39;blue&#39;, [WatchEvent, StarEvent]) s4 = ESpark(&#39;Other&#39;, &#39;red&#39;) s = Stats([s1,s2,s3,s4], store=5, span=.1, stacked=True) console.print(s) . 🌍 Issues PR Follow Other Quota /min 0.0 0.0 0.0 0.0 ━━━━━━━ 0% . You can add events to update counters and sparklines with add_events: . evts = load_sample_events() s.add_events(evts) console.print(s) . 🌍 Issues PR Follow Other Quota /min 11772 ▁▇ 16546 ▁▇ 5991 ▁▇ 6484 ▁ ━━━━━━━ 0% . You can update the progress bar with the update_prog method: . s.update_prog(50) console.print(s) . 🌍 Issues PR Follow Other Quota /min 4076 ▁▇ 5408 ▁▇ 1834 ▁▇ 5998 ▁ ━━━╸━━━ 50% . Here is what the animated version looks like: . . FixedPanel: A panel with fixed height . A key aspect of ghtop is showing events in different panels. We created FixedPanel to allow us to arrange panels in a grid that we can incrementally add events to: . p = FixedPanel(15, box=box.HORIZONTALS, title=&#39;ghtop&#39;) for e in evts: p.append(e) grid([[p,p]]) . ─────────────────── ghtop ─────────────────── ────────────────── ghtop ─────────────────── 📪 dependabo…closed PR #3 o…herzli…&quot;Bump … 📪 dependabo…closed PR #3 …herzli…&quot;Bump … ⭐ dongjun13 pushed 1 commi…dongjun13/2 ⭐ dongjun13 pushed 1 comm…dongjun13/2 ⭐ admmonito…pushed 1 commi…admmonitors/t… ⭐ admmonito…pushed 1 comm…admmonitors/t… ⭐ randomper…pushed 1 commi…randomperson1… ⭐ randomper…pushed 1 comm…randomperson1… ⭐ ahocevar pushed 6 commi…openlayers/ope… ⭐ ahocevar pushed 6 commi…openlayers/op… 🏭 arjmoto created branch …arjmoto/redux-… 🏭 arjmoto created branch…arjmoto/redux-… 💬 stale[bot…created commen…ironha…&quot;This … 💬 stale[bot…created comme…ironha…&quot;This … ⭐ commit-b0…pushed 1 commi…commit-b0t/co… ⭐ commit-b0…pushed 1 comm…commit-b0t/co… ⭐ yakirgot pushed 2 commi…yakirgot/snake ⭐ yakirgot pushed 2 commi…yakirgot/snake 💬 awolf78 created comment…Impulse…&quot;If yo… 💬 awolf78 created commen…Impulse…&quot;If yo… ⭐ kreus7 pushed 1 commit…kreus7/kreusada… ⭐ kreus7 pushed 1 commit…kreus7/kreusad… ⭐ rgripper pushed 1 commi…rgripper/webco… ⭐ rgripper pushed 1 commi…rgripper/webc… 👀 thelittle…started watchi…ritchie46/pol… 👀 thelittle…started watch…ritchie46/pol… 🏭 adrian698 created branch…adrian698/Test 🏭 adrian698 created branc…adrian698/Test ⭐ mergify[b…pushed 2 commi…spbu-coding/6… ⭐ mergify[b…pushed 2 comm…spbu-coding/6… ───────────────────────────────────────────── ──────────────────────────────────────────── . To learn more about our extensions to rich see these docs. . A demo of ghtop animations . Putting all of this together, we get the following results: . 4 Panels with a sparkline for different types of events: . . single panel with a sparkline . . To learn more about ghtop, see the docs. . Interesting python features used . While making these docs, we used the following python features that at least one person we demoed it to found interesting or didn&#39;t know about. If you have been using python for sometime, you might know about all or most of these features: . yield from . Generators are a powerful feature of python, which are especially useful for iterating through large datasets lazily. . dequeue . f-strings .",
            "url": "https://patrick-hanley.github.io/thought_experiments/ghtop",
            "relUrl": "/ghtop",
            "date": " • Jan 29, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://patrick-hanley.github.io/thought_experiments/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://patrick-hanley.github.io/thought_experiments/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://patrick-hanley.github.io/thought_experiments/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}